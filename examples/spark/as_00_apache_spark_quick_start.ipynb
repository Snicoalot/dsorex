{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/17 14:14:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"demo\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/remote_home/projects2/dsor-651-examples/examples/spark'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "root_file_path = os.getcwd()\n",
    "root_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (\"sue\", 32),\n",
    "        (\"li\", 3),\n",
    "        (\"bob\", 75),\n",
    "        (\"heo\", 13),\n",
    "    ],\n",
    "    [\"first_name\", \"age\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|first_name|age|\n",
      "+----------+---+\n",
      "|       sue| 32|\n",
      "|        li|  3|\n",
      "|       bob| 75|\n",
      "|       heo| 13|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "df1 = df.withColumn(\n",
    "    \"life_stage\",\n",
    "    when(col(\"age\") < 13, \"child\")\n",
    "    .when(col(\"age\").between(13, 19), \"teenager\")\n",
    "    .otherwise(\"adult\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+\n",
      "|first_name|age|life_stage|\n",
      "+----------+---+----------+\n",
      "|       sue| 32|     adult|\n",
      "|        li|  3|     child|\n",
      "|       bob| 75|     adult|\n",
      "|       heo| 13|  teenager|\n",
      "+----------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|first_name|age|\n",
      "+----------+---+\n",
      "|       sue| 32|\n",
      "|        li|  3|\n",
      "|       bob| 75|\n",
      "|       heo| 13|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+\n",
      "|first_name|age|life_stage|\n",
      "+----------+---+----------+\n",
      "|       sue| 32|     adult|\n",
      "|       bob| 75|     adult|\n",
      "|       heo| 13|  teenager|\n",
      "+----------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.where(col(\"life_stage\").isin([\"teenager\", \"adult\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|avg(age)|\n",
      "+--------+\n",
      "|   30.75|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df1.select(avg(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|life_stage|avg(age)|\n",
      "+----------+--------+\n",
      "|     adult|    53.5|\n",
      "|     child|     3.0|\n",
      "|  teenager|    13.0|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy(\"life_stage\").avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|avg(age)|\n",
      "+--------+\n",
      "|   30.75|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select avg(age) from {df1}\", df1=df1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|life_stage|avg(age)|\n",
      "+----------+--------+\n",
      "|     adult|    53.5|\n",
      "|     child|     3.0|\n",
      "|  teenager|    13.0|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select life_stage, avg(age) from {df1} group by life_stage\", df1=df1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1.write.saveAsTable(\"some_people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO some_people VALUES ('frank', 4, 'child')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+\n",
      "|first_name|age|life_stage|\n",
      "+----------+---+----------+\n",
      "|       heo| 13|  teenager|\n",
      "|     frank|  4|     child|\n",
      "|       bob| 75|     adult|\n",
      "|       sue| 32|     adult|\n",
      "|        li|  3|     child|\n",
      "+----------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from some_people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----------+\n",
      "|first_name|age|life_stage|\n",
      "+----------+---+----------+\n",
      "|       heo| 13|  teenager|\n",
      "+----------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from some_people where life_stage='teenager'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = spark.sparkContext.textFile(os.path.join(root_file_path,\"some_text.txt\"))\n",
    "\n",
    "counts = (\n",
    "    text_file.flatMap(lambda line: line.split(\" \"))\n",
    "    .map(lambda word: (word, 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('there', 1),\n",
       " ('are', 2),\n",
       " ('these', 1),\n",
       " ('more', 1),\n",
       " ('in', 1),\n",
       " ('words', 3),\n",
       " ('english', 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.142000\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "from operator import add\n",
    "\n",
    "partitions = 4\n",
    "n = 100000 * partitions\n",
    "\n",
    "def f(_: int) -> float:\n",
    "    x = random() * 2 - 1\n",
    "    y = random() * 2 - 1\n",
    "    return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
    "\n",
    "count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 count: 3\n",
      "2 count: 3\n",
      "3 count: 1\n",
      "4 count: 1\n",
      "5 count: 5\n",
      "6 count: 2\n",
      "7 count: 1\n",
      "8 count: 1\n",
      "777 count: 1\n"
     ]
    }
   ],
   "source": [
    "from pyspark import RDD\n",
    "from typing import Tuple\n",
    "\n",
    "lines = spark.read.text(os.path.join(root_file_path,\"some_numbers.txt\")).rdd.map(lambda r: r[0])\n",
    "sortedCount: RDD[Tuple[int, int]] = lines.flatMap(lambda x: x.split(' ')) \\\n",
    "    .map(lambda x: (int(x), 1)) \\\n",
    "    .sortByKey().reduceByKey(lambda a, b: a + b)\n",
    "# This is just a demo on how to bring all the sorted data back to a single node.\n",
    "# In reality, we wouldn't want to collect all the data to the driver node.\n",
    "output = sortedCount.collect()\n",
    "for (num, unitcount) in output:\n",
    "    print(f\"{num} count: {unitcount}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Iterable, Tuple\n",
    "from pyspark.resultiterable import ResultIterable\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def computeContribs(urls: ResultIterable[str], rank: float) -> Iterable[Tuple[str, float]]:\n",
    "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n",
    "\n",
    "\n",
    "def parseNeighbors(urls: str) -> Tuple[str, str]:\n",
    "    \"\"\"Parses a urls pair string into urls pair.\"\"\"\n",
    "    parts = re.split(r'\\s+', urls)\n",
    "    return parts[0], parts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/17 14:16:43 WARN BlockManager: Block rdd_78_0 already exists on this machine; not re-adding it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://digg.com has rank: 0.3625.\n",
      "http://myspace.com has rank: 1.2125.\n",
      "http://slashdot.org has rank: 0.7875.\n",
      "http://www.google.com has rank: 0.7875.\n"
     ]
    }
   ],
   "source": [
    "# Loads in input file. It should be in format of:\n",
    "#     URL         neighbor URL\n",
    "#     URL         neighbor URL\n",
    "#     URL         neighbor URL\n",
    "#     ...\n",
    "lines = spark.read.text(os.path.join(root_file_path,\"page_rank_data.txt\")).rdd.map(lambda r: r[0])\n",
    "\n",
    "# Loads all URLs from input file and initialize their neighbors.\n",
    "links = lines.map(lambda urls: parseNeighbors(urls)).distinct().groupByKey().cache()\n",
    "\n",
    "# Loads all URLs with other URL(s) link to from input file and initialize ranks of them to one.\n",
    "ranks = links.map(lambda url_neighbors: (url_neighbors[0], 1.0))\n",
    "\n",
    "# Calculates and updates URL ranks continuously using PageRank algorithm.\n",
    "iteration_count = 1\n",
    "for iteration in range(iteration_count):\n",
    "    # Calculates URL contributions to the rank of other URLs.\n",
    "    contribs = links.join(ranks).flatMap(lambda url_urls_rank: computeContribs(\n",
    "        url_urls_rank[1][0], url_urls_rank[1][1]  # type: ignore[arg-type]\n",
    "    ))\n",
    "\n",
    "    # Re-calculates URL ranks based on neighbor contributions.\n",
    "    ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)\n",
    "\n",
    "# Collects all URL ranks and dump them to console.\n",
    "for (link, rank) in ranks.collect():\n",
    "    print(\"%s has rank: %s.\" % (link, rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('http://www.google.com', 0.25),\n",
       " ('http://digg.com', 0.25),\n",
       " ('http://myspace.com', 0.25),\n",
       " ('http://slashdot.org', 0.25),\n",
       " ('http://www.google.com', 0.5),\n",
       " ('http://slashdot.org', 0.5),\n",
       " ('http://myspace.com', 1.0)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contribs.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
