{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/opt/homebrew/Cellar/apache-spark/3.4.1/libexec/./bin/spark-submit'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m----> 3\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdemo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Projects/dsor-651-examples/.venv/lib/python3.11/site-packages/pyspark/sql/session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n","File \u001b[0;32m~/Projects/dsor-651-examples/.venv/lib/python3.11/site-packages/pyspark/context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n","File \u001b[0;32m~/Projects/dsor-651-examples/.venv/lib/python3.11/site-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n","File \u001b[0;32m~/Projects/dsor-651-examples/.venv/lib/python3.11/site-packages/pyspark/context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n","File \u001b[0;32m~/Projects/dsor-651-examples/.venv/lib/python3.11/site-packages/pyspark/java_gateway.py:97\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         signal\u001b[38;5;241m.\u001b[39msignal(signal\u001b[38;5;241m.\u001b[39mSIGINT, signal\u001b[38;5;241m.\u001b[39mSIG_IGN)\n\u001b[1;32m     96\u001b[0m     popen_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreexec_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m preexec_func\n\u001b[0;32m---> 97\u001b[0m     proc \u001b[38;5;241m=\u001b[39m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# preexec_fn not supported on Windows\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     proc \u001b[38;5;241m=\u001b[39m Popen(command, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpopen_kwargs)\n","File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n","File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py:1955\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1953\u001b[0m     err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[1;32m   1954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1955\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1956\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/opt/homebrew/Cellar/apache-spark/3.4.1/libexec/./bin/spark-submit'"]}],"source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName(\"demo\").getOrCreate()"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import os\n","\n","data_file_path = os.path.abspath(\"../data/Lottery_Powerball_Winning_Numbers__Beginning_2010.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f94a90bf-017e-4549-b278-0c1e31314be2","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["df1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/1297906913.RANLY.NEI/test_data_Lottery_Powerball_Winning_Numbers__Beginning_2010.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"3db29762-24bf-4c57-be9a-3740b1b1c4ee","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[2]: [&#39;__class__&#39;,\n"," &#39;__delattr__&#39;,\n"," &#39;__dict__&#39;,\n"," &#39;__dir__&#39;,\n"," &#39;__doc__&#39;,\n"," &#39;__eq__&#39;,\n"," &#39;__format__&#39;,\n"," &#39;__ge__&#39;,\n"," &#39;__getattr__&#39;,\n"," &#39;__getattribute__&#39;,\n"," &#39;__getitem__&#39;,\n"," &#39;__gt__&#39;,\n"," &#39;__hash__&#39;,\n"," &#39;__init__&#39;,\n"," &#39;__init_subclass__&#39;,\n"," &#39;__le__&#39;,\n"," &#39;__lt__&#39;,\n"," &#39;__module__&#39;,\n"," &#39;__ne__&#39;,\n"," &#39;__new__&#39;,\n"," &#39;__reduce__&#39;,\n"," &#39;__reduce_ex__&#39;,\n"," &#39;__repr__&#39;,\n"," &#39;__setattr__&#39;,\n"," &#39;__sizeof__&#39;,\n"," &#39;__str__&#39;,\n"," &#39;__subclasshook__&#39;,\n"," &#39;__weakref__&#39;,\n"," &#39;_collect_as_arrow&#39;,\n"," &#39;_jcols&#39;,\n"," &#39;_jdf&#39;,\n"," &#39;_jmap&#39;,\n"," &#39;_joinAsOf&#39;,\n"," &#39;_jseq&#39;,\n"," &#39;_lazy_rdd&#39;,\n"," &#39;_repr_html_&#39;,\n"," &#39;_sc&#39;,\n"," &#39;_schema&#39;,\n"," &#39;_sort_cols&#39;,\n"," &#39;_support_repr_html&#39;,\n"," &#39;_to_corrected_pandas_type&#39;,\n"," &#39;agg&#39;,\n"," &#39;alias&#39;,\n"," &#39;approxQuantile&#39;,\n"," &#39;cache&#39;,\n"," &#39;checkpoint&#39;,\n"," &#39;coalesce&#39;,\n"," &#39;colRegex&#39;,\n"," &#39;collect&#39;,\n"," &#39;columns&#39;,\n"," &#39;corr&#39;,\n"," &#39;count&#39;,\n"," &#39;cov&#39;,\n"," &#39;createGlobalTempView&#39;,\n"," &#39;createOrReplaceGlobalTempView&#39;,\n"," &#39;createOrReplaceTempView&#39;,\n"," &#39;createTempView&#39;,\n"," &#39;crossJoin&#39;,\n"," &#39;crosstab&#39;,\n"," &#39;cube&#39;,\n"," &#39;describe&#39;,\n"," &#39;display&#39;,\n"," &#39;distinct&#39;,\n"," &#39;drop&#39;,\n"," &#39;dropDuplicates&#39;,\n"," &#39;drop_duplicates&#39;,\n"," &#39;dropna&#39;,\n"," &#39;dtypes&#39;,\n"," &#39;exceptAll&#39;,\n"," &#39;explain&#39;,\n"," &#39;fillna&#39;,\n"," &#39;filter&#39;,\n"," &#39;first&#39;,\n"," &#39;foreach&#39;,\n"," &#39;foreachPartition&#39;,\n"," &#39;freqItems&#39;,\n"," &#39;groupBy&#39;,\n"," &#39;groupby&#39;,\n"," &#39;head&#39;,\n"," &#39;hint&#39;,\n"," &#39;inputFiles&#39;,\n"," &#39;intersect&#39;,\n"," &#39;intersectAll&#39;,\n"," &#39;isLocal&#39;,\n"," &#39;isStreaming&#39;,\n"," &#39;is_cached&#39;,\n"," &#39;join&#39;,\n"," &#39;limit&#39;,\n"," &#39;localCheckpoint&#39;,\n"," &#39;mapInPandas&#39;,\n"," &#39;na&#39;,\n"," &#39;orderBy&#39;,\n"," &#39;persist&#39;,\n"," &#39;printSchema&#39;,\n"," &#39;randomSplit&#39;,\n"," &#39;rdd&#39;,\n"," &#39;registerTempTable&#39;,\n"," &#39;repartition&#39;,\n"," &#39;repartitionByRange&#39;,\n"," &#39;replace&#39;,\n"," &#39;rollup&#39;,\n"," &#39;sameSemantics&#39;,\n"," &#39;sample&#39;,\n"," &#39;sampleBy&#39;,\n"," &#39;schema&#39;,\n"," &#39;select&#39;,\n"," &#39;selectExpr&#39;,\n"," &#39;semanticHash&#39;,\n"," &#39;show&#39;,\n"," &#39;sort&#39;,\n"," &#39;sortWithinPartitions&#39;,\n"," &#39;sql_ctx&#39;,\n"," &#39;stat&#39;,\n"," &#39;storageLevel&#39;,\n"," &#39;subtract&#39;,\n"," &#39;summary&#39;,\n"," &#39;tail&#39;,\n"," &#39;take&#39;,\n"," &#39;toDF&#39;,\n"," &#39;toJSON&#39;,\n"," &#39;toLocalIterator&#39;,\n"," &#39;toPandas&#39;,\n"," &#39;to_koalas&#39;,\n"," &#39;to_pandas_on_spark&#39;,\n"," &#39;transform&#39;,\n"," &#39;union&#39;,\n"," &#39;unionAll&#39;,\n"," &#39;unionByName&#39;,\n"," &#39;unpersist&#39;,\n"," &#39;where&#39;,\n"," &#39;withColumn&#39;,\n"," &#39;withColumnRenamed&#39;,\n"," &#39;withColumns&#39;,\n"," &#39;withMetadata&#39;,\n"," &#39;withWatermark&#39;,\n"," &#39;write&#39;,\n"," &#39;writeStream&#39;,\n"," &#39;writeTo&#39;]</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[2]: [&#39;__class__&#39;,\n &#39;__delattr__&#39;,\n &#39;__dict__&#39;,\n &#39;__dir__&#39;,\n &#39;__doc__&#39;,\n &#39;__eq__&#39;,\n &#39;__format__&#39;,\n &#39;__ge__&#39;,\n &#39;__getattr__&#39;,\n &#39;__getattribute__&#39;,\n &#39;__getitem__&#39;,\n &#39;__gt__&#39;,\n &#39;__hash__&#39;,\n &#39;__init__&#39;,\n &#39;__init_subclass__&#39;,\n &#39;__le__&#39;,\n &#39;__lt__&#39;,\n &#39;__module__&#39;,\n &#39;__ne__&#39;,\n &#39;__new__&#39;,\n &#39;__reduce__&#39;,\n &#39;__reduce_ex__&#39;,\n &#39;__repr__&#39;,\n &#39;__setattr__&#39;,\n &#39;__sizeof__&#39;,\n &#39;__str__&#39;,\n &#39;__subclasshook__&#39;,\n &#39;__weakref__&#39;,\n &#39;_collect_as_arrow&#39;,\n &#39;_jcols&#39;,\n &#39;_jdf&#39;,\n &#39;_jmap&#39;,\n &#39;_joinAsOf&#39;,\n &#39;_jseq&#39;,\n &#39;_lazy_rdd&#39;,\n &#39;_repr_html_&#39;,\n &#39;_sc&#39;,\n &#39;_schema&#39;,\n &#39;_sort_cols&#39;,\n &#39;_support_repr_html&#39;,\n &#39;_to_corrected_pandas_type&#39;,\n &#39;agg&#39;,\n &#39;alias&#39;,\n &#39;approxQuantile&#39;,\n &#39;cache&#39;,\n &#39;checkpoint&#39;,\n &#39;coalesce&#39;,\n &#39;colRegex&#39;,\n &#39;collect&#39;,\n &#39;columns&#39;,\n &#39;corr&#39;,\n &#39;count&#39;,\n &#39;cov&#39;,\n &#39;createGlobalTempView&#39;,\n &#39;createOrReplaceGlobalTempView&#39;,\n &#39;createOrReplaceTempView&#39;,\n &#39;createTempView&#39;,\n &#39;crossJoin&#39;,\n &#39;crosstab&#39;,\n &#39;cube&#39;,\n &#39;describe&#39;,\n &#39;display&#39;,\n &#39;distinct&#39;,\n &#39;drop&#39;,\n &#39;dropDuplicates&#39;,\n &#39;drop_duplicates&#39;,\n &#39;dropna&#39;,\n &#39;dtypes&#39;,\n &#39;exceptAll&#39;,\n &#39;explain&#39;,\n &#39;fillna&#39;,\n &#39;filter&#39;,\n &#39;first&#39;,\n &#39;foreach&#39;,\n &#39;foreachPartition&#39;,\n &#39;freqItems&#39;,\n &#39;groupBy&#39;,\n &#39;groupby&#39;,\n &#39;head&#39;,\n &#39;hint&#39;,\n &#39;inputFiles&#39;,\n &#39;intersect&#39;,\n &#39;intersectAll&#39;,\n &#39;isLocal&#39;,\n &#39;isStreaming&#39;,\n &#39;is_cached&#39;,\n &#39;join&#39;,\n &#39;limit&#39;,\n &#39;localCheckpoint&#39;,\n &#39;mapInPandas&#39;,\n &#39;na&#39;,\n &#39;orderBy&#39;,\n &#39;persist&#39;,\n &#39;printSchema&#39;,\n &#39;randomSplit&#39;,\n &#39;rdd&#39;,\n &#39;registerTempTable&#39;,\n &#39;repartition&#39;,\n &#39;repartitionByRange&#39;,\n &#39;replace&#39;,\n &#39;rollup&#39;,\n &#39;sameSemantics&#39;,\n &#39;sample&#39;,\n &#39;sampleBy&#39;,\n &#39;schema&#39;,\n &#39;select&#39;,\n &#39;selectExpr&#39;,\n &#39;semanticHash&#39;,\n &#39;show&#39;,\n &#39;sort&#39;,\n &#39;sortWithinPartitions&#39;,\n &#39;sql_ctx&#39;,\n &#39;stat&#39;,\n &#39;storageLevel&#39;,\n &#39;subtract&#39;,\n &#39;summary&#39;,\n &#39;tail&#39;,\n &#39;take&#39;,\n &#39;toDF&#39;,\n &#39;toJSON&#39;,\n &#39;toLocalIterator&#39;,\n &#39;toPandas&#39;,\n &#39;to_koalas&#39;,\n &#39;to_pandas_on_spark&#39;,\n &#39;transform&#39;,\n &#39;union&#39;,\n &#39;unionAll&#39;,\n &#39;unionByName&#39;,\n &#39;unpersist&#39;,\n &#39;where&#39;,\n &#39;withColumn&#39;,\n &#39;withColumnRenamed&#39;,\n &#39;withColumns&#39;,\n &#39;withMetadata&#39;,\n &#39;withWatermark&#39;,\n &#39;write&#39;,\n &#39;writeStream&#39;,\n &#39;writeTo&#39;]</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["dir(df1)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"8499f7b2-3c87-4ab9-aff5-2cdaf63b4e50","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[3]: pyspark.sql.dataframe.DataFrame</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[3]: pyspark.sql.dataframe.DataFrame</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["type(df1)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"84d960e5-3848-44e5-aafc-46ddd525841a","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[4]: pyspark.rdd.RDD</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[4]: pyspark.rdd.RDD</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["type(df1.rdd)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"df427a30-f33b-4a0d-ae7e-0b96e2f56eab","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[5]: b&#39;(1) MapPartitionsRDD[3661] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[3660] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  SQLExecutionRDD[3659] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[3658] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  FileScanRDD[3657] at javaToPython at NativeMethodAccessorImpl.java:0 []&#39;</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[5]: b&#39;(1) MapPartitionsRDD[3661] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[3660] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  SQLExecutionRDD[3659] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[3658] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  FileScanRDD[3657] at javaToPython at NativeMethodAccessorImpl.java:0 []&#39;</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["df1.rdd.toDebugString()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"ec065187-7a2b-4a0d-9db7-750ba02b28f0","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[6]: 1</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[6]: 1</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["df1.rdd.getNumPartitions()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"2dda9800-d1b3-4e7c-9fa2-47c6fb7a45e0","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[7]: pyspark.sql.session.SparkSession</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[7]: pyspark.sql.session.SparkSession</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["type(spark)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"c56065e0-15ed-426f-9d31-437e8846de36","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[8]: [&#39;Builder&#39;,\n"," &#39;__class__&#39;,\n"," &#39;__delattr__&#39;,\n"," &#39;__dict__&#39;,\n"," &#39;__dir__&#39;,\n"," &#39;__doc__&#39;,\n"," &#39;__enter__&#39;,\n"," &#39;__eq__&#39;,\n"," &#39;__exit__&#39;,\n"," &#39;__format__&#39;,\n"," &#39;__ge__&#39;,\n"," &#39;__getattribute__&#39;,\n"," &#39;__gt__&#39;,\n"," &#39;__hash__&#39;,\n"," &#39;__init__&#39;,\n"," &#39;__init_subclass__&#39;,\n"," &#39;__le__&#39;,\n"," &#39;__lt__&#39;,\n"," &#39;__module__&#39;,\n"," &#39;__ne__&#39;,\n"," &#39;__new__&#39;,\n"," &#39;__reduce__&#39;,\n"," &#39;__reduce_ex__&#39;,\n"," &#39;__repr__&#39;,\n"," &#39;__setattr__&#39;,\n"," &#39;__sizeof__&#39;,\n"," &#39;__str__&#39;,\n"," &#39;__subclasshook__&#39;,\n"," &#39;__weakref__&#39;,\n"," &#39;_activeSession&#39;,\n"," &#39;_convert_from_pandas&#39;,\n"," &#39;_createFromLocal&#39;,\n"," &#39;_createFromLocalTrusted&#39;,\n"," &#39;_createFromRDD&#39;,\n"," &#39;_create_dataframe&#39;,\n"," &#39;_create_from_pandas_with_arrow&#39;,\n"," &#39;_create_rdd_from_local_trusted&#39;,\n"," &#39;_create_shell_session&#39;,\n"," &#39;_get_numpy_record_dtype&#39;,\n"," &#39;_get_pandas_num_slices&#39;,\n"," &#39;_inferSchema&#39;,\n"," &#39;_inferSchemaFromList&#39;,\n"," &#39;_instantiatedSession&#39;,\n"," &#39;_jsc&#39;,\n"," &#39;_jsparkSession&#39;,\n"," &#39;_jvm&#39;,\n"," &#39;_jwrapped&#39;,\n"," &#39;_repr_html_&#39;,\n"," &#39;_sc&#39;,\n"," &#39;_wrap_data_schema&#39;,\n"," &#39;_wrapped&#39;,\n"," &#39;_write_to_trusted_path&#39;,\n"," &#39;builder&#39;,\n"," &#39;catalog&#39;,\n"," &#39;conf&#39;,\n"," &#39;createDataFrame&#39;,\n"," &#39;getActiveSession&#39;,\n"," &#39;newSession&#39;,\n"," &#39;range&#39;,\n"," &#39;read&#39;,\n"," &#39;readStream&#39;,\n"," &#39;sparkContext&#39;,\n"," &#39;sql&#39;,\n"," &#39;stop&#39;,\n"," &#39;streams&#39;,\n"," &#39;table&#39;,\n"," &#39;udf&#39;,\n"," &#39;version&#39;]</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[8]: [&#39;Builder&#39;,\n &#39;__class__&#39;,\n &#39;__delattr__&#39;,\n &#39;__dict__&#39;,\n &#39;__dir__&#39;,\n &#39;__doc__&#39;,\n &#39;__enter__&#39;,\n &#39;__eq__&#39;,\n &#39;__exit__&#39;,\n &#39;__format__&#39;,\n &#39;__ge__&#39;,\n &#39;__getattribute__&#39;,\n &#39;__gt__&#39;,\n &#39;__hash__&#39;,\n &#39;__init__&#39;,\n &#39;__init_subclass__&#39;,\n &#39;__le__&#39;,\n &#39;__lt__&#39;,\n &#39;__module__&#39;,\n &#39;__ne__&#39;,\n &#39;__new__&#39;,\n &#39;__reduce__&#39;,\n &#39;__reduce_ex__&#39;,\n &#39;__repr__&#39;,\n &#39;__setattr__&#39;,\n &#39;__sizeof__&#39;,\n &#39;__str__&#39;,\n &#39;__subclasshook__&#39;,\n &#39;__weakref__&#39;,\n &#39;_activeSession&#39;,\n &#39;_convert_from_pandas&#39;,\n &#39;_createFromLocal&#39;,\n &#39;_createFromLocalTrusted&#39;,\n &#39;_createFromRDD&#39;,\n &#39;_create_dataframe&#39;,\n &#39;_create_from_pandas_with_arrow&#39;,\n &#39;_create_rdd_from_local_trusted&#39;,\n &#39;_create_shell_session&#39;,\n &#39;_get_numpy_record_dtype&#39;,\n &#39;_get_pandas_num_slices&#39;,\n &#39;_inferSchema&#39;,\n &#39;_inferSchemaFromList&#39;,\n &#39;_instantiatedSession&#39;,\n &#39;_jsc&#39;,\n &#39;_jsparkSession&#39;,\n &#39;_jvm&#39;,\n &#39;_jwrapped&#39;,\n &#39;_repr_html_&#39;,\n &#39;_sc&#39;,\n &#39;_wrap_data_schema&#39;,\n &#39;_wrapped&#39;,\n &#39;_write_to_trusted_path&#39;,\n &#39;builder&#39;,\n &#39;catalog&#39;,\n &#39;conf&#39;,\n &#39;createDataFrame&#39;,\n &#39;getActiveSession&#39;,\n &#39;newSession&#39;,\n &#39;range&#39;,\n &#39;read&#39;,\n &#39;readStream&#39;,\n &#39;sparkContext&#39;,\n &#39;sql&#39;,\n &#39;stop&#39;,\n &#39;streams&#39;,\n &#39;table&#39;,\n &#39;udf&#39;,\n &#39;version&#39;]</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["dir(spark)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"948008ad-703d-455a-afb0-197bd689b1cc","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Help on SparkSession in module pyspark.sql.session object:\n","\n","class SparkSession(pyspark.sql.pandas.conversion.SparkConversionMixin)\n","  SparkSession(sparkContext, jsparkSession=None)\n","  \n","  The entry point to programming Spark with the Dataset and DataFrame API.\n","  \n","  A SparkSession can be used create :class:`DataFrame`, register :class:`DataFrame` as\n","  tables, execute SQL over tables, cache tables, and read parquet files.\n","  To create a :class:`SparkSession`, use the following builder pattern:\n","  \n","  .. autoattribute:: builder\n","     :annotation:\n","  \n","  Examples\n","  --------\n","  &gt;&gt;&gt; spark = SparkSession.builder \\\n","  ...     .master(&#34;local&#34;) \\\n","  ...     .appName(&#34;Word Count&#34;) \\\n","  ...     .config(&#34;spark.some.config.option&#34;, &#34;some-value&#34;) \\\n","  ...     .getOrCreate()\n","  \n","  &gt;&gt;&gt; from datetime import datetime\n","  &gt;&gt;&gt; from pyspark.sql import Row\n","  &gt;&gt;&gt; spark = SparkSession(sc)\n","  &gt;&gt;&gt; allTypes = sc.parallelize([Row(i=1, s=&#34;string&#34;, d=1.0, l=1,\n","  ...     b=True, list=[1, 2, 3], dict={&#34;s&#34;: 0}, row=Row(a=1),\n","  ...     time=datetime(2014, 8, 1, 14, 1, 5))])\n","  &gt;&gt;&gt; df = allTypes.toDF()\n","  &gt;&gt;&gt; df.createOrReplaceTempView(&#34;allTypes&#34;)\n","  &gt;&gt;&gt; spark.sql(&#39;select i+1, d+1, not b, list[1], dict[&#34;s&#34;], time, row.a &#39;\n","  ...            &#39;from allTypes where b and i &gt; 0&#39;).collect()\n","  [Row((i + 1)=2, (d + 1)=2.0, (NOT b)=False, list[1]=2,         dict[s]=0, time=datetime.datetime(2014, 8, 1, 14, 1, 5), a=1)]\n","  &gt;&gt;&gt; df.rdd.map(lambda x: (x.i, x.s, x.d, x.l, x.b, x.time, x.row.a, x.list)).collect()\n","  [(1, &#39;string&#39;, 1.0, 1, True, datetime.datetime(2014, 8, 1, 14, 1, 5), 1, [1, 2, 3])]\n","  \n","  Method resolution order:\n","      SparkSession\n","      pyspark.sql.pandas.conversion.SparkConversionMixin\n","      builtins.object\n","  \n","  Methods defined here:\n","  \n","  __enter__(self)\n","      Enable &#39;with SparkSession.builder.(...).getOrCreate() as session: app&#39; syntax.\n","      \n","      .. versionadded:: 2.0\n","  \n","  __exit__(self, exc_type, exc_val, exc_tb)\n","      Enable &#39;with SparkSession.builder.(...).getOrCreate() as session: app&#39; syntax.\n","      \n","      Specifically stop the SparkSession on exit of the with block.\n","      \n","      .. versionadded:: 2.0\n","  \n","  __init__(self, sparkContext, jsparkSession=None)\n","      Initialize self.  See help(type(self)) for accurate signature.\n","  \n","  createDataFrame(self, data, schema=None, samplingRatio=None, verifySchema=True)\n","      Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n","      \n","      When ``schema`` is a list of column names, the type of each column\n","      will be inferred from ``data``.\n","      \n","      When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n","      from ``data``, which should be an RDD of either :class:`Row`,\n","      :class:`namedtuple`, or :class:`dict`.\n","      \n","      When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must match\n","      the real data, or an exception will be thrown at runtime. If the given schema is not\n","      :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n","      :class:`pyspark.sql.types.StructType` as its only field, and the field name will be &#34;value&#34;.\n","      Each record will also be wrapped into a tuple, which can be converted to row later.\n","      \n","      If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n","      rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n","      \n","      .. versionadded:: 2.0.0\n","      \n","      .. versionchanged:: 2.1.0\n","         Added verifySchema.\n","      \n","      Parameters\n","      ----------\n","      data : :class:`RDD` or iterable\n","          an RDD of any kind of SQL data representation (:class:`Row`,\n","          :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`, or\n","          :class:`pandas.DataFrame`.\n","      schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n","          a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n","          column names, default is None.  The data type string format equals to\n","          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n","          omit the ``struct&lt;&gt;`` and atomic types use ``typeName()`` as their format, e.g. use\n","          ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`.\n","          We can also use ``int`` as a short name for :class:`pyspark.sql.types.IntegerType`.\n","      samplingRatio : float, optional\n","          the sample ratio of rows used for inferring\n","      verifySchema : bool, optional\n","          verify data types of every row against schema. Enabled by default.\n","      \n","      Returns\n","      -------\n","      :class:`DataFrame`\n","      \n","      Notes\n","      -----\n","      Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n","      \n","      Examples\n","      --------\n","      &gt;&gt;&gt; l = [(&#39;Alice&#39;, 1)]\n","      &gt;&gt;&gt; spark.createDataFrame(l).collect()\n","      [Row(_1=&#39;Alice&#39;, _2=1)]\n","      &gt;&gt;&gt; spark.createDataFrame(l, [&#39;name&#39;, &#39;age&#39;]).collect()\n","      [Row(name=&#39;Alice&#39;, age=1)]\n","      \n","      &gt;&gt;&gt; d = [{&#39;name&#39;: &#39;Alice&#39;, &#39;age&#39;: 1}]\n","      &gt;&gt;&gt; spark.createDataFrame(d).collect()\n","      [Row(age=1, name=&#39;Alice&#39;)]\n","      \n","      &gt;&gt;&gt; rdd = sc.parallelize(l)\n","      &gt;&gt;&gt; spark.createDataFrame(rdd).collect()\n","      [Row(_1=&#39;Alice&#39;, _2=1)]\n","      &gt;&gt;&gt; df = spark.createDataFrame(rdd, [&#39;name&#39;, &#39;age&#39;])\n","      &gt;&gt;&gt; df.collect()\n","      [Row(name=&#39;Alice&#39;, age=1)]\n","      \n","      &gt;&gt;&gt; from pyspark.sql import Row\n","      &gt;&gt;&gt; Person = Row(&#39;name&#39;, &#39;age&#39;)\n","      &gt;&gt;&gt; person = rdd.map(lambda r: Person(*r))\n","      &gt;&gt;&gt; df2 = spark.createDataFrame(person)\n","      &gt;&gt;&gt; df2.collect()\n","      [Row(name=&#39;Alice&#39;, age=1)]\n","      \n","      &gt;&gt;&gt; from pyspark.sql.types import *\n","      &gt;&gt;&gt; schema = StructType([\n","      ...    StructField(&#34;name&#34;, StringType(), True),\n","      ...    StructField(&#34;age&#34;, IntegerType(), True)])\n","      &gt;&gt;&gt; df3 = spark.createDataFrame(rdd, schema)\n","      &gt;&gt;&gt; df3.collect()\n","      [Row(name=&#39;Alice&#39;, age=1)]\n","      \n","      &gt;&gt;&gt; spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n","      [Row(name=&#39;Alice&#39;, age=1)]\n","      &gt;&gt;&gt; spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n","      [Row(0=1, 1=2)]\n","      \n","      &gt;&gt;&gt; spark.createDataFrame(rdd, &#34;a: string, b: int&#34;).collect()\n","      [Row(a=&#39;Alice&#39;, b=1)]\n","      &gt;&gt;&gt; rdd = rdd.map(lambda row: row[1])\n","      &gt;&gt;&gt; spark.createDataFrame(rdd, &#34;int&#34;).collect()\n","      [Row(value=1)]\n","      &gt;&gt;&gt; spark.createDataFrame(rdd, &#34;boolean&#34;).collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n","      Traceback (most recent call last):\n","          ...\n","      Py4JJavaError: ...\n","  \n","  newSession(self)\n","      Returns a new :class:`SparkSession` as new session, that has separate SQLConf,\n","      registered temporary views and UDFs, but shared :class:`SparkContext` and\n","      table cache.\n","      \n","      .. versionadded:: 2.0\n","  \n","  range(self, start, end=None, step=1, numPartitions=None)\n","      Create a :class:`DataFrame` with single :class:`pyspark.sql.types.LongType` column named\n","      ``id``, containing elements in a range from ``start`` to ``end`` (exclusive) with\n","      step value ``step``.\n","      \n","      .. versionadded:: 2.0.0\n","      \n","      Parameters\n","      ----------\n","      start : int\n","          the start value\n","      end : int, optional\n","          the end value (exclusive)\n","      step : int, optional\n","          the incremental step (default: 1)\n","      numPartitions : int, optional\n","          the number of partitions of the DataFrame\n","      \n","      Returns\n","      -------\n","      :class:`DataFrame`\n","      \n","      Examples\n","      --------\n","      &gt;&gt;&gt; spark.range(1, 7, 2).collect()\n","      [Row(id=1), Row(id=3), Row(id=5)]\n","      \n","      If only one argument is specified, it will be used as the end value.\n","      \n","      &gt;&gt;&gt; spark.range(3).collect()\n","      [Row(id=0), Row(id=1), Row(id=2)]\n","  \n","  sql(self, sqlQuery)\n","      Returns a :class:`DataFrame` representing the result of the given query.\n","      \n","      .. versionadded:: 2.0.0\n","      \n","      Returns\n","      -------\n","      :class:`DataFrame`\n","      \n","      Examples\n","      --------\n","      &gt;&gt;&gt; df.createOrReplaceTempView(&#34;table1&#34;)\n","      &gt;&gt;&gt; df2 = spark.sql(&#34;SELECT field1 AS f1, field2 as f2 from table1&#34;)\n","      &gt;&gt;&gt; df2.collect()\n","      [Row(f1=1, f2=&#39;row1&#39;), Row(f1=2, f2=&#39;row2&#39;), Row(f1=3, f2=&#39;row3&#39;)]\n","  \n","  stop(self)\n","      Stop the underlying :class:`SparkContext`.\n","      \n","      .. versionadded:: 2.0\n","  \n","  table(self, tableName)\n","      Returns the specified table as a :class:`DataFrame`.\n","      \n","      .. versionadded:: 2.0.0\n","      \n","      Returns\n","      -------\n","      :class:`DataFrame`\n","      \n","      Examples\n","      --------\n","      &gt;&gt;&gt; df.createOrReplaceTempView(&#34;table1&#34;)\n","      &gt;&gt;&gt; df2 = spark.table(&#34;table1&#34;)\n","      &gt;&gt;&gt; sorted(df.collect()) == sorted(df2.collect())\n","      True\n","  \n","  ----------------------------------------------------------------------\n","  Class methods defined here:\n","  \n","  getActiveSession() from builtins.type\n","      Returns the active :class:`SparkSession` for the current thread, returned by the builder\n","      \n","      .. versionadded:: 3.0.0\n","      \n","      Returns\n","      -------\n","      :class:`SparkSession`\n","          Spark session if an active session exists for the current thread\n","      \n","      Examples\n","      --------\n","      &gt;&gt;&gt; s = SparkSession.getActiveSession()\n","      &gt;&gt;&gt; l = [(&#39;Alice&#39;, 1)]\n","      &gt;&gt;&gt; rdd = s.sparkContext.parallelize(l)\n","      &gt;&gt;&gt; df = s.createDataFrame(rdd, [&#39;name&#39;, &#39;age&#39;])\n","      &gt;&gt;&gt; df.select(&#34;age&#34;).collect()\n","      [Row(age=1)]\n","  \n","  ----------------------------------------------------------------------\n","  Readonly properties defined here:\n","  \n","  catalog\n","      Interface through which the user may create, drop, alter or query underlying\n","      databases, tables, functions, etc.\n","      \n","      .. versionadded:: 2.0.0\n","      \n","      Returns\n","      -------\n","      :class:`Catalog`\n","  \n","  conf\n","      Runtime configuration interface for Spark.\n","      \n","      This is the interface through which the user can get and set all Spark and Hadoop\n","      configurations that are relevant to Spark SQL. When getting the value of a config,\n","      this defaults to the value set in the underlying :class:`SparkContext`, if any.\n","      \n","      Returns\n","      -------\n","      :class:`pyspark.sql.conf.RuntimeConfig`\n","      \n","      .. versionadded:: 2.0\n","  \n","  read\n","      Returns a :class:`DataFrameReader` that can be used to read data\n","      in as a :class:`DataFrame`.\n","      \n","      .. versionadded:: 2.0.0\n","      \n","      Returns\n","      -------\n","      :class:`DataFrameReader`\n","  \n","  readStream\n","      Returns a :class:`DataStreamReader` that can be used to read data streams\n","      as a streaming :class:`DataFrame`.\n","      \n","      .. versionadded:: 2.0.0\n","      \n","      Notes\n","      -----\n","      This API is evolving.\n","      \n","      Returns\n","      -------\n","      :class:`DataStreamReader`\n","  \n","  sparkContext\n","      Returns the underlying :class:`SparkContext`.\n","      \n","      .. versionadded:: 2.0\n","  \n","  streams\n","      Returns a :class:`StreamingQueryManager` that allows managing all the\n","      :class:`StreamingQuery` instances active on `this` context.\n","      \n","      .. versionadded:: 2.0.0\n","      \n","      Notes\n","      -----\n","      This API is evolving.\n","      \n","      Returns\n","      -------\n","      :class:`StreamingQueryManager`\n","  \n","  udf\n","      Returns a :class:`UDFRegistration` for UDF registration.\n","      \n","      .. versionadded:: 2.0.0\n","      \n","      Returns\n","      -------\n","      :class:`UDFRegistration`\n","  \n","  version\n","      The version of Spark on which this application is running.\n","      \n","      .. versionadded:: 2.0\n","  \n","  ----------------------------------------------------------------------\n","  Data and other attributes defined here:\n","  \n","  Builder = &lt;class &#39;pyspark.sql.session.SparkSession.Builder&#39;&gt;\n","      Builder for :class:`SparkSession`.\n","  \n","  builder = &lt;pyspark.sql.session.SparkSession.Builder object&gt;\n","  \n","  ----------------------------------------------------------------------\n","  Data descriptors inherited from pyspark.sql.pandas.conversion.SparkConversionMixin:\n","  \n","  __dict__\n","      dictionary for instance variables (if defined)\n","  \n","  __weakref__\n","      list of weak references to the object (if defined)\n","\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Help on SparkSession in module pyspark.sql.session object:\n\nclass SparkSession(pyspark.sql.pandas.conversion.SparkConversionMixin)\n |  SparkSession(sparkContext, jsparkSession=None)\n |  \n |  The entry point to programming Spark with the Dataset and DataFrame API.\n |  \n |  A SparkSession can be used create :class:`DataFrame`, register :class:`DataFrame` as\n |  tables, execute SQL over tables, cache tables, and read parquet files.\n |  To create a :class:`SparkSession`, use the following builder pattern:\n |  \n |  .. autoattribute:: builder\n |     :annotation:\n |  \n |  Examples\n |  --------\n |  &gt;&gt;&gt; spark = SparkSession.builder \\\n |  ...     .master(&#34;local&#34;) \\\n |  ...     .appName(&#34;Word Count&#34;) \\\n |  ...     .config(&#34;spark.some.config.option&#34;, &#34;some-value&#34;) \\\n |  ...     .getOrCreate()\n |  \n |  &gt;&gt;&gt; from datetime import datetime\n |  &gt;&gt;&gt; from pyspark.sql import Row\n |  &gt;&gt;&gt; spark = SparkSession(sc)\n |  &gt;&gt;&gt; allTypes = sc.parallelize([Row(i=1, s=&#34;string&#34;, d=1.0, l=1,\n |  ...     b=True, list=[1, 2, 3], dict={&#34;s&#34;: 0}, row=Row(a=1),\n |  ...     time=datetime(2014, 8, 1, 14, 1, 5))])\n |  &gt;&gt;&gt; df = allTypes.toDF()\n |  &gt;&gt;&gt; df.createOrReplaceTempView(&#34;allTypes&#34;)\n |  &gt;&gt;&gt; spark.sql(&#39;select i+1, d+1, not b, list[1], dict[&#34;s&#34;], time, row.a &#39;\n |  ...            &#39;from allTypes where b and i &gt; 0&#39;).collect()\n |  [Row((i + 1)=2, (d + 1)=2.0, (NOT b)=False, list[1]=2,         dict[s]=0, time=datetime.datetime(2014, 8, 1, 14, 1, 5), a=1)]\n |  &gt;&gt;&gt; df.rdd.map(lambda x: (x.i, x.s, x.d, x.l, x.b, x.time, x.row.a, x.list)).collect()\n |  [(1, &#39;string&#39;, 1.0, 1, True, datetime.datetime(2014, 8, 1, 14, 1, 5), 1, [1, 2, 3])]\n |  \n |  Method resolution order:\n |      SparkSession\n |      pyspark.sql.pandas.conversion.SparkConversionMixin\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __enter__(self)\n |      Enable &#39;with SparkSession.builder.(...).getOrCreate() as session: app&#39; syntax.\n |      \n |      .. versionadded:: 2.0\n |  \n |  __exit__(self, exc_type, exc_val, exc_tb)\n |      Enable &#39;with SparkSession.builder.(...).getOrCreate() as session: app&#39; syntax.\n |      \n |      Specifically stop the SparkSession on exit of the with block.\n |      \n |      .. versionadded:: 2.0\n |  \n |  __init__(self, sparkContext, jsparkSession=None)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  createDataFrame(self, data, schema=None, samplingRatio=None, verifySchema=True)\n |      Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n |      \n |      When ``schema`` is a list of column names, the type of each column\n |      will be inferred from ``data``.\n |      \n |      When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n |      from ``data``, which should be an RDD of either :class:`Row`,\n |      :class:`namedtuple`, or :class:`dict`.\n |      \n |      When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must match\n |      the real data, or an exception will be thrown at runtime. If the given schema is not\n |      :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n |      :class:`pyspark.sql.types.StructType` as its only field, and the field name will be &#34;value&#34;.\n |      Each record will also be wrapped into a tuple, which can be converted to row later.\n |      \n |      If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n |      rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      .. versionchanged:: 2.1.0\n |         Added verifySchema.\n |      \n |      Parameters\n |      ----------\n |      data : :class:`RDD` or iterable\n |          an RDD of any kind of SQL data representation (:class:`Row`,\n |          :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`, or\n |          :class:`pandas.DataFrame`.\n |      schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n |          a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n |          column names, default is None.  The data type string format equals to\n |          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n |          omit the ``struct&lt;&gt;`` and atomic types use ``typeName()`` as their format, e.g. use\n |          ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`.\n |          We can also use ``int`` as a short name for :class:`pyspark.sql.types.IntegerType`.\n |      samplingRatio : float, optional\n |          the sample ratio of rows used for inferring\n |      verifySchema : bool, optional\n |          verify data types of every row against schema. Enabled by default.\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |      \n |      Notes\n |      -----\n |      Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; l = [(&#39;Alice&#39;, 1)]\n |      &gt;&gt;&gt; spark.createDataFrame(l).collect()\n |      [Row(_1=&#39;Alice&#39;, _2=1)]\n |      &gt;&gt;&gt; spark.createDataFrame(l, [&#39;name&#39;, &#39;age&#39;]).collect()\n |      [Row(name=&#39;Alice&#39;, age=1)]\n |      \n |      &gt;&gt;&gt; d = [{&#39;name&#39;: &#39;Alice&#39;, &#39;age&#39;: 1}]\n |      &gt;&gt;&gt; spark.createDataFrame(d).collect()\n |      [Row(age=1, name=&#39;Alice&#39;)]\n |      \n |      &gt;&gt;&gt; rdd = sc.parallelize(l)\n |      &gt;&gt;&gt; spark.createDataFrame(rdd).collect()\n |      [Row(_1=&#39;Alice&#39;, _2=1)]\n |      &gt;&gt;&gt; df = spark.createDataFrame(rdd, [&#39;name&#39;, &#39;age&#39;])\n |      &gt;&gt;&gt; df.collect()\n |      [Row(name=&#39;Alice&#39;, age=1)]\n |      \n |      &gt;&gt;&gt; from pyspark.sql import Row\n |      &gt;&gt;&gt; Person = Row(&#39;name&#39;, &#39;age&#39;)\n |      &gt;&gt;&gt; person = rdd.map(lambda r: Person(*r))\n |      &gt;&gt;&gt; df2 = spark.createDataFrame(person)\n |      &gt;&gt;&gt; df2.collect()\n |      [Row(name=&#39;Alice&#39;, age=1)]\n |      \n |      &gt;&gt;&gt; from pyspark.sql.types import *\n |      &gt;&gt;&gt; schema = StructType([\n |      ...    StructField(&#34;name&#34;, StringType(), True),\n |      ...    StructField(&#34;age&#34;, IntegerType(), True)])\n |      &gt;&gt;&gt; df3 = spark.createDataFrame(rdd, schema)\n |      &gt;&gt;&gt; df3.collect()\n |      [Row(name=&#39;Alice&#39;, age=1)]\n |      \n |      &gt;&gt;&gt; spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n |      [Row(name=&#39;Alice&#39;, age=1)]\n |      &gt;&gt;&gt; spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n |      [Row(0=1, 1=2)]\n |      \n |      &gt;&gt;&gt; spark.createDataFrame(rdd, &#34;a: string, b: int&#34;).collect()\n |      [Row(a=&#39;Alice&#39;, b=1)]\n |      &gt;&gt;&gt; rdd = rdd.map(lambda row: row[1])\n |      &gt;&gt;&gt; spark.createDataFrame(rdd, &#34;int&#34;).collect()\n |      [Row(value=1)]\n |      &gt;&gt;&gt; spark.createDataFrame(rdd, &#34;boolean&#34;).collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n |      Traceback (most recent call last):\n |          ...\n |      Py4JJavaError: ...\n |  \n |  newSession(self)\n |      Returns a new :class:`SparkSession` as new session, that has separate SQLConf,\n |      registered temporary views and UDFs, but shared :class:`SparkContext` and\n |      table cache.\n |      \n |      .. versionadded:: 2.0\n |  \n |  range(self, start, end=None, step=1, numPartitions=None)\n |      Create a :class:`DataFrame` with single :class:`pyspark.sql.types.LongType` column named\n |      ``id``, containing elements in a range from ``start`` to ``end`` (exclusive) with\n |      step value ``step``.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Parameters\n |      ----------\n |      start : int\n |          the start value\n |      end : int, optional\n |          the end value (exclusive)\n |      step : int, optional\n |          the incremental step (default: 1)\n |      numPartitions : int, optional\n |          the number of partitions of the DataFrame\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; spark.range(1, 7, 2).collect()\n |      [Row(id=1), Row(id=3), Row(id=5)]\n |      \n |      If only one argument is specified, it will be used as the end value.\n |      \n |      &gt;&gt;&gt; spark.range(3).collect()\n |      [Row(id=0), Row(id=1), Row(id=2)]\n |  \n |  sql(self, sqlQuery)\n |      Returns a :class:`DataFrame` representing the result of the given query.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df.createOrReplaceTempView(&#34;table1&#34;)\n |      &gt;&gt;&gt; df2 = spark.sql(&#34;SELECT field1 AS f1, field2 as f2 from table1&#34;)\n |      &gt;&gt;&gt; df2.collect()\n |      [Row(f1=1, f2=&#39;row1&#39;), Row(f1=2, f2=&#39;row2&#39;), Row(f1=3, f2=&#39;row3&#39;)]\n |  \n |  stop(self)\n |      Stop the underlying :class:`SparkContext`.\n |      \n |      .. versionadded:: 2.0\n |  \n |  table(self, tableName)\n |      Returns the specified table as a :class:`DataFrame`.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; df.createOrReplaceTempView(&#34;table1&#34;)\n |      &gt;&gt;&gt; df2 = spark.table(&#34;table1&#34;)\n |      &gt;&gt;&gt; sorted(df.collect()) == sorted(df2.collect())\n |      True\n |  \n |  ----------------------------------------------------------------------\n |  Class methods defined here:\n |  \n |  getActiveSession() from builtins.type\n |      Returns the active :class:`SparkSession` for the current thread, returned by the builder\n |      \n |      .. versionadded:: 3.0.0\n |      \n |      Returns\n |      -------\n |      :class:`SparkSession`\n |          Spark session if an active session exists for the current thread\n |      \n |      Examples\n |      --------\n |      &gt;&gt;&gt; s = SparkSession.getActiveSession()\n |      &gt;&gt;&gt; l = [(&#39;Alice&#39;, 1)]\n |      &gt;&gt;&gt; rdd = s.sparkContext.parallelize(l)\n |      &gt;&gt;&gt; df = s.createDataFrame(rdd, [&#39;name&#39;, &#39;age&#39;])\n |      &gt;&gt;&gt; df.select(&#34;age&#34;).collect()\n |      [Row(age=1)]\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties defined here:\n |  \n |  catalog\n |      Interface through which the user may create, drop, alter or query underlying\n |      databases, tables, functions, etc.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Returns\n |      -------\n |      :class:`Catalog`\n |  \n |  conf\n |      Runtime configuration interface for Spark.\n |      \n |      This is the interface through which the user can get and set all Spark and Hadoop\n |      configurations that are relevant to Spark SQL. When getting the value of a config,\n |      this defaults to the value set in the underlying :class:`SparkContext`, if any.\n |      \n |      Returns\n |      -------\n |      :class:`pyspark.sql.conf.RuntimeConfig`\n |      \n |      .. versionadded:: 2.0\n |  \n |  read\n |      Returns a :class:`DataFrameReader` that can be used to read data\n |      in as a :class:`DataFrame`.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Returns\n |      -------\n |      :class:`DataFrameReader`\n |  \n |  readStream\n |      Returns a :class:`DataStreamReader` that can be used to read data streams\n |      as a streaming :class:`DataFrame`.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Notes\n |      -----\n |      This API is evolving.\n |      \n |      Returns\n |      -------\n |      :class:`DataStreamReader`\n |  \n |  sparkContext\n |      Returns the underlying :class:`SparkContext`.\n |      \n |      .. versionadded:: 2.0\n |  \n |  streams\n |      Returns a :class:`StreamingQueryManager` that allows managing all the\n |      :class:`StreamingQuery` instances active on `this` context.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Notes\n |      -----\n |      This API is evolving.\n |      \n |      Returns\n |      -------\n |      :class:`StreamingQueryManager`\n |  \n |  udf\n |      Returns a :class:`UDFRegistration` for UDF registration.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Returns\n |      -------\n |      :class:`UDFRegistration`\n |  \n |  version\n |      The version of Spark on which this application is running.\n |      \n |      .. versionadded:: 2.0\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  Builder = &lt;class &#39;pyspark.sql.session.SparkSession.Builder&#39;&gt;\n |      Builder for :class:`SparkSession`.\n |  \n |  builder = &lt;pyspark.sql.session.SparkSession.Builder object&gt;\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from pyspark.sql.pandas.conversion.SparkConversionMixin:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["help(spark)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"50429a5f-81f2-4b8a-a0cd-9af9948809d1","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[10]: [&#39;PACKAGE_EXTENSIONS&#39;,\n"," &#39;__class__&#39;,\n"," &#39;__delattr__&#39;,\n"," &#39;__dict__&#39;,\n"," &#39;__dir__&#39;,\n"," &#39;__doc__&#39;,\n"," &#39;__enter__&#39;,\n"," &#39;__eq__&#39;,\n"," &#39;__exit__&#39;,\n"," &#39;__format__&#39;,\n"," &#39;__ge__&#39;,\n"," &#39;__getattribute__&#39;,\n"," &#39;__getnewargs__&#39;,\n"," &#39;__gt__&#39;,\n"," &#39;__hash__&#39;,\n"," &#39;__init__&#39;,\n"," &#39;__init_subclass__&#39;,\n"," &#39;__le__&#39;,\n"," &#39;__lt__&#39;,\n"," &#39;__module__&#39;,\n"," &#39;__ne__&#39;,\n"," &#39;__new__&#39;,\n"," &#39;__reduce__&#39;,\n"," &#39;__reduce_ex__&#39;,\n"," &#39;__repr__&#39;,\n"," &#39;__setattr__&#39;,\n"," &#39;__sizeof__&#39;,\n"," &#39;__str__&#39;,\n"," &#39;__subclasshook__&#39;,\n"," &#39;__weakref__&#39;,\n"," &#39;_accumulatorServer&#39;,\n"," &#39;_active_spark_context&#39;,\n"," &#39;_assert_on_driver&#39;,\n"," &#39;_batchSize&#39;,\n"," &#39;_callsite&#39;,\n"," &#39;_checkpointFile&#39;,\n"," &#39;_conf&#39;,\n"," &#39;_dictToJavaMap&#39;,\n"," &#39;_do_init&#39;,\n"," &#39;_encryption_enabled&#39;,\n"," &#39;_ensure_initialized&#39;,\n"," &#39;_gateway&#39;,\n"," &#39;_getJavaStorageLevel&#39;,\n"," &#39;_initialize_context&#39;,\n"," &#39;_javaAccumulator&#39;,\n"," &#39;_jsc&#39;,\n"," &#39;_jvm&#39;,\n"," &#39;_lock&#39;,\n"," &#39;_next_accum_id&#39;,\n"," &#39;_pickled_broadcast_vars&#39;,\n"," &#39;_python_includes&#39;,\n"," &#39;_repr_html_&#39;,\n"," &#39;_serialize_to_jvm&#39;,\n"," &#39;_temp_dir&#39;,\n"," &#39;_unbatched_serializer&#39;,\n"," &#39;accumulator&#39;,\n"," &#39;addClusterWideLibraryToPath&#39;,\n"," &#39;addFile&#39;,\n"," &#39;addIsolatedLibraryPath&#39;,\n"," &#39;addPyFile&#39;,\n"," &#39;appName&#39;,\n"," &#39;applicationId&#39;,\n"," &#39;binaryFiles&#39;,\n"," &#39;binaryRecords&#39;,\n"," &#39;broadcast&#39;,\n"," &#39;cancelAllJobs&#39;,\n"," &#39;cancelJobGroup&#39;,\n"," &#39;defaultMinPartitions&#39;,\n"," &#39;defaultParallelism&#39;,\n"," &#39;dump_profiles&#39;,\n"," &#39;emptyRDD&#39;,\n"," &#39;environment&#39;,\n"," &#39;getCheckpointDir&#39;,\n"," &#39;getConf&#39;,\n"," &#39;getLocalProperty&#39;,\n"," &#39;getOrCreate&#39;,\n"," &#39;hadoopFile&#39;,\n"," &#39;hadoopRDD&#39;,\n"," &#39;init_batched_serializer&#39;,\n"," &#39;master&#39;,\n"," &#39;newAPIHadoopFile&#39;,\n"," &#39;newAPIHadoopRDD&#39;,\n"," &#39;parallelize&#39;,\n"," &#39;pickleFile&#39;,\n"," &#39;profiler_collector&#39;,\n"," &#39;pythonExec&#39;,\n"," &#39;pythonVer&#39;,\n"," &#39;range&#39;,\n"," &#39;resources&#39;,\n"," &#39;runJob&#39;,\n"," &#39;sequenceFile&#39;,\n"," &#39;serializer&#39;,\n"," &#39;setCheckpointDir&#39;,\n"," &#39;setJobDescription&#39;,\n"," &#39;setJobGroup&#39;,\n"," &#39;setLocalProperty&#39;,\n"," &#39;setLogLevel&#39;,\n"," &#39;setSystemProperty&#39;,\n"," &#39;show_profiles&#39;,\n"," &#39;sparkHome&#39;,\n"," &#39;sparkUser&#39;,\n"," &#39;startTime&#39;,\n"," &#39;statusTracker&#39;,\n"," &#39;stop&#39;,\n"," &#39;textFile&#39;,\n"," &#39;uiWebUrl&#39;,\n"," &#39;union&#39;,\n"," &#39;version&#39;,\n"," &#39;wholeTextFiles&#39;]</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[10]: [&#39;PACKAGE_EXTENSIONS&#39;,\n &#39;__class__&#39;,\n &#39;__delattr__&#39;,\n &#39;__dict__&#39;,\n &#39;__dir__&#39;,\n &#39;__doc__&#39;,\n &#39;__enter__&#39;,\n &#39;__eq__&#39;,\n &#39;__exit__&#39;,\n &#39;__format__&#39;,\n &#39;__ge__&#39;,\n &#39;__getattribute__&#39;,\n &#39;__getnewargs__&#39;,\n &#39;__gt__&#39;,\n &#39;__hash__&#39;,\n &#39;__init__&#39;,\n &#39;__init_subclass__&#39;,\n &#39;__le__&#39;,\n &#39;__lt__&#39;,\n &#39;__module__&#39;,\n &#39;__ne__&#39;,\n &#39;__new__&#39;,\n &#39;__reduce__&#39;,\n &#39;__reduce_ex__&#39;,\n &#39;__repr__&#39;,\n &#39;__setattr__&#39;,\n &#39;__sizeof__&#39;,\n &#39;__str__&#39;,\n &#39;__subclasshook__&#39;,\n &#39;__weakref__&#39;,\n &#39;_accumulatorServer&#39;,\n &#39;_active_spark_context&#39;,\n &#39;_assert_on_driver&#39;,\n &#39;_batchSize&#39;,\n &#39;_callsite&#39;,\n &#39;_checkpointFile&#39;,\n &#39;_conf&#39;,\n &#39;_dictToJavaMap&#39;,\n &#39;_do_init&#39;,\n &#39;_encryption_enabled&#39;,\n &#39;_ensure_initialized&#39;,\n &#39;_gateway&#39;,\n &#39;_getJavaStorageLevel&#39;,\n &#39;_initialize_context&#39;,\n &#39;_javaAccumulator&#39;,\n &#39;_jsc&#39;,\n &#39;_jvm&#39;,\n &#39;_lock&#39;,\n &#39;_next_accum_id&#39;,\n &#39;_pickled_broadcast_vars&#39;,\n &#39;_python_includes&#39;,\n &#39;_repr_html_&#39;,\n &#39;_serialize_to_jvm&#39;,\n &#39;_temp_dir&#39;,\n &#39;_unbatched_serializer&#39;,\n &#39;accumulator&#39;,\n &#39;addClusterWideLibraryToPath&#39;,\n &#39;addFile&#39;,\n &#39;addIsolatedLibraryPath&#39;,\n &#39;addPyFile&#39;,\n &#39;appName&#39;,\n &#39;applicationId&#39;,\n &#39;binaryFiles&#39;,\n &#39;binaryRecords&#39;,\n &#39;broadcast&#39;,\n &#39;cancelAllJobs&#39;,\n &#39;cancelJobGroup&#39;,\n &#39;defaultMinPartitions&#39;,\n &#39;defaultParallelism&#39;,\n &#39;dump_profiles&#39;,\n &#39;emptyRDD&#39;,\n &#39;environment&#39;,\n &#39;getCheckpointDir&#39;,\n &#39;getConf&#39;,\n &#39;getLocalProperty&#39;,\n &#39;getOrCreate&#39;,\n &#39;hadoopFile&#39;,\n &#39;hadoopRDD&#39;,\n &#39;init_batched_serializer&#39;,\n &#39;master&#39;,\n &#39;newAPIHadoopFile&#39;,\n &#39;newAPIHadoopRDD&#39;,\n &#39;parallelize&#39;,\n &#39;pickleFile&#39;,\n &#39;profiler_collector&#39;,\n &#39;pythonExec&#39;,\n &#39;pythonVer&#39;,\n &#39;range&#39;,\n &#39;resources&#39;,\n &#39;runJob&#39;,\n &#39;sequenceFile&#39;,\n &#39;serializer&#39;,\n &#39;setCheckpointDir&#39;,\n &#39;setJobDescription&#39;,\n &#39;setJobGroup&#39;,\n &#39;setLocalProperty&#39;,\n &#39;setLogLevel&#39;,\n &#39;setSystemProperty&#39;,\n &#39;show_profiles&#39;,\n &#39;sparkHome&#39;,\n &#39;sparkUser&#39;,\n &#39;startTime&#39;,\n &#39;statusTracker&#39;,\n &#39;stop&#39;,\n &#39;textFile&#39;,\n &#39;uiWebUrl&#39;,\n &#39;union&#39;,\n &#39;version&#39;,\n &#39;wholeTextFiles&#39;]</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["dir(spark.sparkContext)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b1c2adec-5286-4b8a-81c9-fb0d0493fb4b","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Help on method repartition in module pyspark.sql.dataframe:\n","\n","repartition(numPartitions, *cols) method of pyspark.sql.dataframe.DataFrame instance\n","    Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n","    resulting :class:`DataFrame` is hash partitioned.\n","    \n","    .. versionadded:: 1.3.0\n","    \n","    Parameters\n","    ----------\n","    numPartitions : int\n","        can be an int to specify the target number of partitions or a Column.\n","        If it is a Column, it will be used as the first partitioning column. If not specified,\n","        the default number of partitions is used.\n","    cols : str or :class:`Column`\n","        partitioning columns.\n","    \n","        .. versionchanged:: 1.6\n","           Added optional arguments to specify the partitioning columns. Also made numPartitions\n","           optional if partitioning columns are specified.\n","    \n","    Examples\n","    --------\n","    &gt;&gt;&gt; df.repartition(10).rdd.getNumPartitions()\n","    10\n","    &gt;&gt;&gt; data = df.union(df).repartition(&#34;age&#34;)\n","    &gt;&gt;&gt; data.show()\n","    +---+-----+\n","age| name|\n","    +---+-----+\n","  2|Alice|\n","  5|  Bob|\n","  2|Alice|\n","  5|  Bob|\n","    +---+-----+\n","    &gt;&gt;&gt; data = data.repartition(7, &#34;age&#34;)\n","    &gt;&gt;&gt; data.show()\n","    +---+-----+\n","age| name|\n","    +---+-----+\n","  2|Alice|\n","  5|  Bob|\n","  2|Alice|\n","  5|  Bob|\n","    +---+-----+\n","    &gt;&gt;&gt; data.rdd.getNumPartitions()\n","    7\n","    &gt;&gt;&gt; data = data.repartition(3, &#34;name&#34;, &#34;age&#34;)\n","    &gt;&gt;&gt; data.show()\n","    +---+-----+\n","age| name|\n","    +---+-----+\n","  5|  Bob|\n","  5|  Bob|\n","  2|Alice|\n","  2|Alice|\n","    +---+-----+\n","\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Help on method repartition in module pyspark.sql.dataframe:\n\nrepartition(numPartitions, *cols) method of pyspark.sql.dataframe.DataFrame instance\n    Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n    resulting :class:`DataFrame` is hash partitioned.\n    \n    .. versionadded:: 1.3.0\n    \n    Parameters\n    ----------\n    numPartitions : int\n        can be an int to specify the target number of partitions or a Column.\n        If it is a Column, it will be used as the first partitioning column. If not specified,\n        the default number of partitions is used.\n    cols : str or :class:`Column`\n        partitioning columns.\n    \n        .. versionchanged:: 1.6\n           Added optional arguments to specify the partitioning columns. Also made numPartitions\n           optional if partitioning columns are specified.\n    \n    Examples\n    --------\n    &gt;&gt;&gt; df.repartition(10).rdd.getNumPartitions()\n    10\n    &gt;&gt;&gt; data = df.union(df).repartition(&#34;age&#34;)\n    &gt;&gt;&gt; data.show()\n    +---+-----+\n    |age| name|\n    +---+-----+\n    |  2|Alice|\n    |  5|  Bob|\n    |  2|Alice|\n    |  5|  Bob|\n    +---+-----+\n    &gt;&gt;&gt; data = data.repartition(7, &#34;age&#34;)\n    &gt;&gt;&gt; data.show()\n    +---+-----+\n    |age| name|\n    +---+-----+\n    |  2|Alice|\n    |  5|  Bob|\n    |  2|Alice|\n    |  5|  Bob|\n    +---+-----+\n    &gt;&gt;&gt; data.rdd.getNumPartitions()\n    7\n    &gt;&gt;&gt; data = data.repartition(3, &#34;name&#34;, &#34;age&#34;)\n    &gt;&gt;&gt; data.show()\n    +---+-----+\n    |age| name|\n    +---+-----+\n    |  5|  Bob|\n    |  5|  Bob|\n    |  2|Alice|\n    |  2|Alice|\n    +---+-----+\n\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["help(df1.repartition)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"4b737a50-d8d5-4c59-ad0e-1c958e4cd9ec","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["df2 = df1.repartition(4)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"3967db16-86ed-4e42-9ffe-f24701eaf662","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[13]: 4</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[13]: 4</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["df2.rdd.getNumPartitions()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"a7d3457d-c1cb-4f8e-b8c6-7c15c35a2359","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[14]: b&#39;(4) MapPartitionsRDD[3669] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[3668] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  SQLExecutionRDD[3667] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  ShuffledRowRDD[3666] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n +-(1) MapPartitionsRDD[3665] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n    |  MapPartitionsRDD[3664] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n    |  MapPartitionsRDD[3663] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n    |  FileScanRDD[3662] at javaToPython at NativeMethodAccessorImpl.java:0 []&#39;</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[14]: b&#39;(4) MapPartitionsRDD[3669] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[3668] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  SQLExecutionRDD[3667] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  ShuffledRowRDD[3666] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n +-(1) MapPartitionsRDD[3665] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n    |  MapPartitionsRDD[3664] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n    |  MapPartitionsRDD[3663] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n    |  FileScanRDD[3662] at javaToPython at NativeMethodAccessorImpl.java:0 []&#39;</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["df2.rdd.toDebugString()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b717d45a-314d-47d8-bd7a-d19cdede4293","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[15]: 1</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[15]: 1</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["df1.rdd.getNumPartitions()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"ed256c71-2750-425a-8de4-8ef0dd1abfc7","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["import timeit"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"1059dccb-1bed-4175-a2e3-5a55d67b2ec1","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[17]: [&#39;Draw Date&#39;, &#39;Winning Numbers&#39;, &#39;Multiplier&#39;]</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[17]: [&#39;Draw Date&#39;, &#39;Winning Numbers&#39;, &#39;Multiplier&#39;]</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["df2.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"a0ef407f-1550-4b4e-a393-71ca0575252b","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[18]: [Row(Draw Date=&#39;03/28/2012&#39;, Winning Numbers=&#39;11 16 29 50 58 33&#39;, Multiplier=None),\n"," Row(Draw Date=&#39;08/22/2020&#39;, Winning Numbers=&#39;19 30 36 42 66 14&#39;, Multiplier=&#39;3&#39;),\n"," Row(Draw Date=&#39;12/28/2022&#39;, Winning Numbers=&#39;26 32 38 45 56 01&#39;, Multiplier=&#39;2&#39;),\n"," Row(Draw Date=&#39;10/29/2014&#39;, Winning Numbers=&#39;25 28 48 57 59 16&#39;, Multiplier=&#39;3&#39;),\n"," Row(Draw Date=&#39;01/01/2022&#39;, Winning Numbers=&#39;06 12 39 48 50 07&#39;, Multiplier=&#39;2&#39;)]</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[18]: [Row(Draw Date=&#39;03/28/2012&#39;, Winning Numbers=&#39;11 16 29 50 58 33&#39;, Multiplier=None),\n Row(Draw Date=&#39;08/22/2020&#39;, Winning Numbers=&#39;19 30 36 42 66 14&#39;, Multiplier=&#39;3&#39;),\n Row(Draw Date=&#39;12/28/2022&#39;, Winning Numbers=&#39;26 32 38 45 56 01&#39;, Multiplier=&#39;2&#39;),\n Row(Draw Date=&#39;10/29/2014&#39;, Winning Numbers=&#39;25 28 48 57 59 16&#39;, Multiplier=&#39;3&#39;),\n Row(Draw Date=&#39;01/01/2022&#39;, Winning Numbers=&#39;06 12 39 48 50 07&#39;, Multiplier=&#39;2&#39;)]</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["df2.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"1e7a6261-732b-4176-9b29-967ac499ca78","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">+-------+----------+-----------------+------------------+\n","summary| Draw Date|  Winning Numbers|        Multiplier|\n","+-------+----------+-----------------+------------------+\n","  count|      1494|             1494|              1284|\n","   mean|      null|             null| 2.825545171339564|\n"," stddev|      null|             null|1.2049916052850873|\n","    min|01/01/2011|01 02 03 07 39 25|                10|\n","    max|12/31/2022|52 58 59 64 66 09|                 5|\n","+-------+----------+-----------------+------------------+\n","\n","+-------+----------+-----------------+------------------+\n","summary| Draw Date|  Winning Numbers|        Multiplier|\n","+-------+----------+-----------------+------------------+\n","  count|      1494|             1494|              1284|\n","   mean|      null|             null| 2.825545171339564|\n"," stddev|      null|             null|1.2049916052850873|\n","    min|01/01/2011|01 02 03 07 39 25|                10|\n","    max|12/31/2022|52 58 59 64 66 09|                 5|\n","+-------+----------+-----------------+------------------+\n","\n","+-------+----------+-----------------+------------------+\n","summary| Draw Date|  Winning Numbers|        Multiplier|\n","+-------+----------+-----------------+------------------+\n","  count|      1494|             1494|              1284|\n","   mean|      null|             null| 2.825545171339564|\n"," stddev|      null|             null|1.2049916052850873|\n","    min|01/01/2011|01 02 03 07 39 25|                10|\n","    max|12/31/2022|52 58 59 64 66 09|                 5|\n","+-------+----------+-----------------+------------------+\n","\n","+-------+----------+-----------------+------------------+\n","summary| Draw Date|  Winning Numbers|        Multiplier|\n","+-------+----------+-----------------+------------------+\n","  count|      1494|             1494|              1284|\n","   mean|      null|             null| 2.825545171339564|\n"," stddev|      null|             null|1.2049916052850873|\n","    min|01/01/2011|01 02 03 07 39 25|                10|\n","    max|12/31/2022|52 58 59 64 66 09|                 5|\n","+-------+----------+-----------------+------------------+\n","\n","+-------+----------+-----------------+------------------+\n","summary| Draw Date|  Winning Numbers|        Multiplier|\n","+-------+----------+-----------------+------------------+\n","  count|      1494|             1494|              1284|\n","   mean|      null|             null| 2.825545171339564|\n"," stddev|      null|             null|1.2049916052850873|\n","    min|01/01/2011|01 02 03 07 39 25|                10|\n","    max|12/31/2022|52 58 59 64 66 09|                 5|\n","+-------+----------+-----------------+------------------+\n","\n","Out[19]: 2.511526749935001</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">+-------+----------+-----------------+------------------+\n|summary| Draw Date|  Winning Numbers|        Multiplier|\n+-------+----------+-----------------+------------------+\n|  count|      1494|             1494|              1284|\n|   mean|      null|             null| 2.825545171339564|\n| stddev|      null|             null|1.2049916052850873|\n|    min|01/01/2011|01 02 03 07 39 25|                10|\n|    max|12/31/2022|52 58 59 64 66 09|                 5|\n+-------+----------+-----------------+------------------+\n\n+-------+----------+-----------------+------------------+\n|summary| Draw Date|  Winning Numbers|        Multiplier|\n+-------+----------+-----------------+------------------+\n|  count|      1494|             1494|              1284|\n|   mean|      null|             null| 2.825545171339564|\n| stddev|      null|             null|1.2049916052850873|\n|    min|01/01/2011|01 02 03 07 39 25|                10|\n|    max|12/31/2022|52 58 59 64 66 09|                 5|\n+-------+----------+-----------------+------------------+\n\n+-------+----------+-----------------+------------------+\n|summary| Draw Date|  Winning Numbers|        Multiplier|\n+-------+----------+-----------------+------------------+\n|  count|      1494|             1494|              1284|\n|   mean|      null|             null| 2.825545171339564|\n| stddev|      null|             null|1.2049916052850873|\n|    min|01/01/2011|01 02 03 07 39 25|                10|\n|    max|12/31/2022|52 58 59 64 66 09|                 5|\n+-------+----------+-----------------+------------------+\n\n+-------+----------+-----------------+------------------+\n|summary| Draw Date|  Winning Numbers|        Multiplier|\n+-------+----------+-----------------+------------------+\n|  count|      1494|             1494|              1284|\n|   mean|      null|             null| 2.825545171339564|\n| stddev|      null|             null|1.2049916052850873|\n|    min|01/01/2011|01 02 03 07 39 25|                10|\n|    max|12/31/2022|52 58 59 64 66 09|                 5|\n+-------+----------+-----------------+------------------+\n\n+-------+----------+-----------------+------------------+\n|summary| Draw Date|  Winning Numbers|        Multiplier|\n+-------+----------+-----------------+------------------+\n|  count|      1494|             1494|              1284|\n|   mean|      null|             null| 2.825545171339564|\n| stddev|      null|             null|1.2049916052850873|\n|    min|01/01/2011|01 02 03 07 39 25|                10|\n|    max|12/31/2022|52 58 59 64 66 09|                 5|\n+-------+----------+-----------------+------------------+\n\nOut[19]: 2.511526749935001</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["timeit.timeit(lambda : df2.describe().show(),number=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"31578203-7bd7-425d-81b7-1405245c6947","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">+-------+----------+-----------------+------------------+\n","summary| Draw Date|  Winning Numbers|        Multiplier|\n","+-------+----------+-----------------+------------------+\n","  count|      1494|             1494|              1284|\n","   mean|      null|             null| 2.825545171339564|\n"," stddev|      null|             null|1.2049916052850869|\n","    min|01/01/2011|01 02 03 07 39 25|                10|\n","    max|12/31/2022|52 58 59 64 66 09|                 5|\n","+-------+----------+-----------------+------------------+\n","\n","+-------+----------+-----------------+------------------+\n","summary| Draw Date|  Winning Numbers|        Multiplier|\n","+-------+----------+-----------------+------------------+\n","  count|      1494|             1494|              1284|\n","   mean|      null|             null| 2.825545171339564|\n"," stddev|      null|             null|1.2049916052850869|\n","    min|01/01/2011|01 02 03 07 39 25|                10|\n","    max|12/31/2022|52 58 59 64 66 09|                 5|\n","+-------+----------+-----------------+------------------+\n","\n","+-------+----------+-----------------+------------------+\n","summary| Draw Date|  Winning Numbers|        Multiplier|\n","+-------+----------+-----------------+------------------+\n","  count|      1494|             1494|              1284|\n","   mean|      null|             null| 2.825545171339564|\n"," stddev|      null|             null|1.2049916052850869|\n","    min|01/01/2011|01 02 03 07 39 25|                10|\n","    max|12/31/2022|52 58 59 64 66 09|                 5|\n","+-------+----------+-----------------+------------------+\n","\n","+-------+----------+-----------------+------------------+\n","summary| Draw Date|  Winning Numbers|        Multiplier|\n","+-------+----------+-----------------+------------------+\n","  count|      1494|             1494|              1284|\n","   mean|      null|             null| 2.825545171339564|\n"," stddev|      null|             null|1.2049916052850869|\n","    min|01/01/2011|01 02 03 07 39 25|                10|\n","    max|12/31/2022|52 58 59 64 66 09|                 5|\n","+-------+----------+-----------------+------------------+\n","\n","+-------+----------+-----------------+------------------+\n","summary| Draw Date|  Winning Numbers|        Multiplier|\n","+-------+----------+-----------------+------------------+\n","  count|      1494|             1494|              1284|\n","   mean|      null|             null| 2.825545171339564|\n"," stddev|      null|             null|1.2049916052850869|\n","    min|01/01/2011|01 02 03 07 39 25|                10|\n","    max|12/31/2022|52 58 59 64 66 09|                 5|\n","+-------+----------+-----------------+------------------+\n","\n","Out[20]: 1.1206668260274455</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">+-------+----------+-----------------+------------------+\n|summary| Draw Date|  Winning Numbers|        Multiplier|\n+-------+----------+-----------------+------------------+\n|  count|      1494|             1494|              1284|\n|   mean|      null|             null| 2.825545171339564|\n| stddev|      null|             null|1.2049916052850869|\n|    min|01/01/2011|01 02 03 07 39 25|                10|\n|    max|12/31/2022|52 58 59 64 66 09|                 5|\n+-------+----------+-----------------+------------------+\n\n+-------+----------+-----------------+------------------+\n|summary| Draw Date|  Winning Numbers|        Multiplier|\n+-------+----------+-----------------+------------------+\n|  count|      1494|             1494|              1284|\n|   mean|      null|             null| 2.825545171339564|\n| stddev|      null|             null|1.2049916052850869|\n|    min|01/01/2011|01 02 03 07 39 25|                10|\n|    max|12/31/2022|52 58 59 64 66 09|                 5|\n+-------+----------+-----------------+------------------+\n\n+-------+----------+-----------------+------------------+\n|summary| Draw Date|  Winning Numbers|        Multiplier|\n+-------+----------+-----------------+------------------+\n|  count|      1494|             1494|              1284|\n|   mean|      null|             null| 2.825545171339564|\n| stddev|      null|             null|1.2049916052850869|\n|    min|01/01/2011|01 02 03 07 39 25|                10|\n|    max|12/31/2022|52 58 59 64 66 09|                 5|\n+-------+----------+-----------------+------------------+\n\n+-------+----------+-----------------+------------------+\n|summary| Draw Date|  Winning Numbers|        Multiplier|\n+-------+----------+-----------------+------------------+\n|  count|      1494|             1494|              1284|\n|   mean|      null|             null| 2.825545171339564|\n| stddev|      null|             null|1.2049916052850869|\n|    min|01/01/2011|01 02 03 07 39 25|                10|\n|    max|12/31/2022|52 58 59 64 66 09|                 5|\n+-------+----------+-----------------+------------------+\n\n+-------+----------+-----------------+------------------+\n|summary| Draw Date|  Winning Numbers|        Multiplier|\n+-------+----------+-----------------+------------------+\n|  count|      1494|             1494|              1284|\n|   mean|      null|             null| 2.825545171339564|\n| stddev|      null|             null|1.2049916052850869|\n|    min|01/01/2011|01 02 03 07 39 25|                10|\n|    max|12/31/2022|52 58 59 64 66 09|                 5|\n+-------+----------+-----------------+------------------+\n\nOut[20]: 1.1206668260274455</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["timeit.timeit(lambda : df1.describe().show(),number=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"9b4fb79e-b5c2-45b2-8a73-73a9a06f389a","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">+----------+-----------------+----------+\n"," Draw Date|  Winning Numbers|Multiplier|\n","+----------+-----------------+----------+\n","10/22/2014|29 30 40 42 50 16|         2|\n","06/12/2019|05 35 38 42 57 13|         2|\n","03/07/2015|34 36 38 42 50 33|         4|\n","01/03/2018|02 18 37 39 42 12|         3|\n","12/29/2018|12 42 51 53 62 25|         2|\n","11/30/2019|15 35 42 63 68 18|         4|\n","02/27/2019|21 31 42 49 59 23|         5|\n","04/21/2012|06 08 20 42 51 16|      null|\n","09/10/2022|38 42 56 68 69 04|         2|\n","07/01/2017|19 42 45 48 53 16|         3|\n","10/10/2018|08 23 27 42 60 07|         3|\n","10/12/2022|14 30 41 42 59 06|         5|\n","05/12/2018|22 42 45 55 56 14|         3|\n","10/02/2013|04 06 25 42 51 17|      null|\n","07/17/2013|01 22 34 38 42 17|      null|\n","06/30/2018|03 09 20 42 61 24|         2|\n","01/04/2017|16 17 29 41 42 04|         3|\n","06/12/2013|16 22 23 42 55 32|      null|\n","09/25/2010|08 16 27 35 42 30|         2|\n","04/16/2014|34 39 42 44 59 08|         3|\n","+----------+-----------------+----------+\n","only showing top 20 rows\n","\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">+----------+-----------------+----------+\n| Draw Date|  Winning Numbers|Multiplier|\n+----------+-----------------+----------+\n|10/22/2014|29 30 40 42 50 16|         2|\n|06/12/2019|05 35 38 42 57 13|         2|\n|03/07/2015|34 36 38 42 50 33|         4|\n|01/03/2018|02 18 37 39 42 12|         3|\n|12/29/2018|12 42 51 53 62 25|         2|\n|11/30/2019|15 35 42 63 68 18|         4|\n|02/27/2019|21 31 42 49 59 23|         5|\n|04/21/2012|06 08 20 42 51 16|      null|\n|09/10/2022|38 42 56 68 69 04|         2|\n|07/01/2017|19 42 45 48 53 16|         3|\n|10/10/2018|08 23 27 42 60 07|         3|\n|10/12/2022|14 30 41 42 59 06|         5|\n|05/12/2018|22 42 45 55 56 14|         3|\n|10/02/2013|04 06 25 42 51 17|      null|\n|07/17/2013|01 22 34 38 42 17|      null|\n|06/30/2018|03 09 20 42 61 24|         2|\n|01/04/2017|16 17 29 41 42 04|         3|\n|06/12/2013|16 22 23 42 55 32|      null|\n|09/25/2010|08 16 27 35 42 30|         2|\n|04/16/2014|34 39 42 44 59 08|         3|\n+----------+-----------------+----------+\nonly showing top 20 rows\n\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["df2.filter(df2[\"Winning Numbers\"].contains(\"42\")).show()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"77d4fcb7-2a43-4fe6-b736-8918ef654c17","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">+----------+-----------------+----------+\n"," Draw Date|  Winning Numbers|Multiplier|\n","+----------+-----------------+----------+\n","10/31/2020|02 06 40 42 55 24|         3|\n","01/27/2021|17 33 35 42 52 09|         3|\n","03/17/2021|34 38 42 61 62 19|         2|\n","03/20/2021|01 06 22 42 61 04|         3|\n","08/22/2020|19 30 36 42 66 14|         3|\n","05/20/2020|18 34 40 42 50 09|         2|\n","05/16/2020|08 12 26 39 42 11|         2|\n","05/09/2020|12 18 42 48 65 19|         5|\n","04/11/2020|22 29 30 42 47 17|         3|\n","03/25/2020|05 09 27 39 42 16|         2|\n","12/11/2019|24 29 42 44 63 10|         4|\n","12/07/2019|18 42 53 62 66 25|         3|\n","11/30/2019|15 35 42 63 68 18|         4|\n","09/07/2019|11 20 41 42 56 06|         2|\n","06/12/2019|05 35 38 42 57 13|         2|\n","06/08/2019|09 13 42 48 60 18|         2|\n","05/29/2019|03 32 34 42 61 07|         2|\n","02/27/2019|21 31 42 49 59 23|         5|\n","01/02/2019|08 12 42 46 56 12|         2|\n","12/29/2018|12 42 51 53 62 25|         2|\n","+----------+-----------------+----------+\n","only showing top 20 rows\n","\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">+----------+-----------------+----------+\n| Draw Date|  Winning Numbers|Multiplier|\n+----------+-----------------+----------+\n|10/31/2020|02 06 40 42 55 24|         3|\n|01/27/2021|17 33 35 42 52 09|         3|\n|03/17/2021|34 38 42 61 62 19|         2|\n|03/20/2021|01 06 22 42 61 04|         3|\n|08/22/2020|19 30 36 42 66 14|         3|\n|05/20/2020|18 34 40 42 50 09|         2|\n|05/16/2020|08 12 26 39 42 11|         2|\n|05/09/2020|12 18 42 48 65 19|         5|\n|04/11/2020|22 29 30 42 47 17|         3|\n|03/25/2020|05 09 27 39 42 16|         2|\n|12/11/2019|24 29 42 44 63 10|         4|\n|12/07/2019|18 42 53 62 66 25|         3|\n|11/30/2019|15 35 42 63 68 18|         4|\n|09/07/2019|11 20 41 42 56 06|         2|\n|06/12/2019|05 35 38 42 57 13|         2|\n|06/08/2019|09 13 42 48 60 18|         2|\n|05/29/2019|03 32 34 42 61 07|         2|\n|02/27/2019|21 31 42 49 59 23|         5|\n|01/02/2019|08 12 42 46 56 12|         2|\n|12/29/2018|12 42 51 53 62 25|         2|\n+----------+-----------------+----------+\nonly showing top 20 rows\n\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["df1.filter(df1[\"Winning Numbers\"].contains(\"42\")).show()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f31ce932-0e3b-4a4f-bd0c-664935ad6d8b","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["c = df2[\"Winning Numbers\"].contains(\"42\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"4bed2f74-5010-4e7b-a925-b27219e57060","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[24]: pyspark.sql.column.Column</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[24]: pyspark.sql.column.Column</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["type(c)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"71aeb66f-80a8-4070-bd40-a545a4072fe2","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["df1_subset = df1.filter(df1[\"Winning Numbers\"].contains(\"42\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"4688cb9f-5477-4773-8bb4-ada622d45cc1","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[26]: pyspark.sql.dataframe.DataFrame</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[26]: pyspark.sql.dataframe.DataFrame</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["type(df1_subset)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"457ba9c0-be40-4bea-be29-c0b73cca9481","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[27]: b&#39;(1) MapPartitionsRDD[3774] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[3773] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  SQLExecutionRDD[3772] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[3771] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[3770] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  FileScanRDD[3769] at javaToPython at NativeMethodAccessorImpl.java:0 []&#39;</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[27]: b&#39;(1) MapPartitionsRDD[3774] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[3773] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  SQLExecutionRDD[3772] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[3771] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[3770] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  FileScanRDD[3769] at javaToPython at NativeMethodAccessorImpl.java:0 []&#39;</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["df1_subset.rdd.toDebugString()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"776ad925-24f8-4cc2-91d4-e90344700f03","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"},{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n","<span class=\"ansi-red-fg\">IllegalArgumentException</span>                  Traceback (most recent call last)\n","<span class=\"ansi-green-fg\">&lt;command-1560458&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n","<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>df1<span class=\"ansi-blue-fg\">.</span>approxQuantile<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Multiplier&#34;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0.5</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">0.25</span><span class=\"ansi-blue-fg\">)</span>\n","\n","<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">approxQuantile</span><span class=\"ansi-blue-fg\">(self, col, probabilities, relativeError)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">   2474</span>         relativeError <span class=\"ansi-blue-fg\">=</span> float<span class=\"ansi-blue-fg\">(</span>relativeError<span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">   2475</span> \n","<span class=\"ansi-green-fg\">-&gt; 2476</span><span class=\"ansi-red-fg\">         </span>jaq <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>stat<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>approxQuantile<span class=\"ansi-blue-fg\">(</span>col<span class=\"ansi-blue-fg\">,</span> probabilities<span class=\"ansi-blue-fg\">,</span> relativeError<span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">   2477</span>         jaq_list <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">[</span>list<span class=\"ansi-blue-fg\">(</span>j<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">for</span> j <span class=\"ansi-green-fg\">in</span> jaq<span class=\"ansi-blue-fg\">]</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">   2478</span>         <span class=\"ansi-green-fg\">return</span> jaq_list<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-green-fg\">if</span> isStr <span class=\"ansi-green-fg\">else</span> jaq_list\n","\n","<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n","<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n","<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n","</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n","<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n","\n","<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">    121</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n","<span class=\"ansi-green-fg\">--&gt; 123</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">    124</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n","<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>                 <span class=\"ansi-green-fg\">raise</span>\n","\n","<span class=\"ansi-red-fg\">IllegalArgumentException</span>: requirement failed: Quantile calculation for column Multiplier with data type StringType is not supported.</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">IllegalArgumentException</span>                  Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1560458&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>df1<span class=\"ansi-blue-fg\">.</span>approxQuantile<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Multiplier&#34;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0.5</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">0.25</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">approxQuantile</span><span class=\"ansi-blue-fg\">(self, col, probabilities, relativeError)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2474</span>         relativeError <span class=\"ansi-blue-fg\">=</span> float<span class=\"ansi-blue-fg\">(</span>relativeError<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2475</span> \n<span class=\"ansi-green-fg\">-&gt; 2476</span><span class=\"ansi-red-fg\">         </span>jaq <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>stat<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>approxQuantile<span class=\"ansi-blue-fg\">(</span>col<span class=\"ansi-blue-fg\">,</span> probabilities<span class=\"ansi-blue-fg\">,</span> relativeError<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2477</span>         jaq_list <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">[</span>list<span class=\"ansi-blue-fg\">(</span>j<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">for</span> j <span class=\"ansi-green-fg\">in</span> jaq<span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2478</span>         <span class=\"ansi-green-fg\">return</span> jaq_list<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-green-fg\">if</span> isStr <span class=\"ansi-green-fg\">else</span> jaq_list\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    121</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n<span class=\"ansi-green-fg\">--&gt; 123</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    124</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>                 <span class=\"ansi-green-fg\">raise</span>\n\n<span class=\"ansi-red-fg\">IllegalArgumentException</span>: requirement failed: Quantile calculation for column Multiplier with data type StringType is not supported.</div>","errorSummary":"<span class=\"ansi-red-fg\">IllegalArgumentException</span>: requirement failed: Quantile calculation for column Multiplier with data type StringType is not supported.","errorTraceType":"html","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["df1.approxQuantile(\"Multiplier\",[0.5], 0.25)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"94004616-1468-4440-9964-d41510bd3be5","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">root\n","-- Draw Date: string (nullable = true)\n","-- Winning Numbers: string (nullable = true)\n","-- Multiplier: string (nullable = true)\n","\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">root\n |-- Draw Date: string (nullable = true)\n |-- Winning Numbers: string (nullable = true)\n |-- Multiplier: string (nullable = true)\n\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["df2.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"fca2b61a-dd1e-4a6e-80d8-a22b01662858","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["df1_v2 = df1.withColumn(\"Multiplier\",df1[\"Multiplier\"].cast(\"int\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"2336723a-4823-4b69-9c16-d87a412199ef","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["df2_v2 = df2.withColumn(\"Multiplier\",df2[\"Multiplier\"].cast(\"int\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6235942a-6b32-4e3f-a063-890402cbc485","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">root\n","-- Draw Date: string (nullable = true)\n","-- Winning Numbers: string (nullable = true)\n","-- Multiplier: integer (nullable = true)\n","\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">root\n |-- Draw Date: string (nullable = true)\n |-- Winning Numbers: string (nullable = true)\n |-- Multiplier: integer (nullable = true)\n\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["df1_v2.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"8a25a4ac-a5ea-4c6e-89e3-c1d927a21faf","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[33]: [2.0]</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[33]: [2.0]</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["df1_v2.approxQuantile(\"Multiplier\",[0.5], 0.25)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"aad5e60e-f81c-4fc9-a6c7-2ac93f98507b","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[34]: [2.0]</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[34]: [2.0]</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["df2_v2.approxQuantile(\"Multiplier\",[0.5], 0.25)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"81f08a02-6c30-45cd-a7d2-e6ebb3f4cf8d","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Help on method approxQuantile in module pyspark.sql.dataframe:\n","\n","approxQuantile(col, probabilities, relativeError) method of pyspark.sql.dataframe.DataFrame instance\n","    Calculates the approximate quantiles of numerical columns of a\n","    :class:`DataFrame`.\n","    \n","    The result of this algorithm has the following deterministic bound:\n","    If the :class:`DataFrame` has N elements and if we request the quantile at\n","    probability `p` up to error `err`, then the algorithm will return\n","    a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n","    close to (p * N). More precisely,\n","    \n","      floor((p - err) * N) &lt;= rank(x) &lt;= ceil((p + err) * N).\n","    \n","    This method implements a variation of the Greenwald-Khanna\n","    algorithm (with some speed optimizations). The algorithm was first\n","    present in [[https://doi.org/10.1145/375663.375670\n","    Space-efficient Online Computation of Quantile Summaries]]\n","    by Greenwald and Khanna.\n","    \n","    Note that null values will be ignored in numerical columns before calculation.\n","    For columns only containing null values, an empty list is returned.\n","    \n","    .. versionadded:: 2.0.0\n","    \n","    Parameters\n","    ----------\n","    col: str, tuple or list\n","        Can be a single column name, or a list of names for multiple columns.\n","    \n","        .. versionchanged:: 2.2\n","           Added support for multiple columns.\n","    probabilities : list or tuple\n","        a list of quantile probabilities\n","        Each number must belong to [0, 1].\n","        For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n","    relativeError : float\n","        The relative target precision to achieve\n","        (&gt;= 0). If set to zero, the exact quantiles are computed, which\n","        could be very expensive. Note that values greater than 1 are\n","        accepted but give the same result as 1.\n","    \n","    Returns\n","    -------\n","    list\n","        the approximate quantiles at the given probabilities. If\n","        the input `col` is a string, the output is a list of floats. If the\n","        input `col` is a list or tuple of strings, the output is also a\n","        list, but each element in it is a list of floats, i.e., the output\n","        is a list of list of floats.\n","\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Help on method approxQuantile in module pyspark.sql.dataframe:\n\napproxQuantile(col, probabilities, relativeError) method of pyspark.sql.dataframe.DataFrame instance\n    Calculates the approximate quantiles of numerical columns of a\n    :class:`DataFrame`.\n    \n    The result of this algorithm has the following deterministic bound:\n    If the :class:`DataFrame` has N elements and if we request the quantile at\n    probability `p` up to error `err`, then the algorithm will return\n    a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n    close to (p * N). More precisely,\n    \n      floor((p - err) * N) &lt;= rank(x) &lt;= ceil((p + err) * N).\n    \n    This method implements a variation of the Greenwald-Khanna\n    algorithm (with some speed optimizations). The algorithm was first\n    present in [[https://doi.org/10.1145/375663.375670\n    Space-efficient Online Computation of Quantile Summaries]]\n    by Greenwald and Khanna.\n    \n    Note that null values will be ignored in numerical columns before calculation.\n    For columns only containing null values, an empty list is returned.\n    \n    .. versionadded:: 2.0.0\n    \n    Parameters\n    ----------\n    col: str, tuple or list\n        Can be a single column name, or a list of names for multiple columns.\n    \n        .. versionchanged:: 2.2\n           Added support for multiple columns.\n    probabilities : list or tuple\n        a list of quantile probabilities\n        Each number must belong to [0, 1].\n        For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n    relativeError : float\n        The relative target precision to achieve\n        (&gt;= 0). If set to zero, the exact quantiles are computed, which\n        could be very expensive. Note that values greater than 1 are\n        accepted but give the same result as 1.\n    \n    Returns\n    -------\n    list\n        the approximate quantiles at the given probabilities. If\n        the input `col` is a string, the output is a list of floats. If the\n        input `col` is a list or tuple of strings, the output is also a\n        list, but each element in it is a list of floats, i.e., the output\n        is a list of list of floats.\n\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["help(df1.approxQuantile)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"903d2632-4c05-46ef-ac79-87a3dae4e83e","showTitle":false,"title":""}},"outputs":[],"source":[]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"Apache Spark Getting Started","notebookOrigID":1559579,"widgets":{}},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
