{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","24/04/17 14:27:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","24/04/17 14:27:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"]}],"source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName(\"demo\").getOrCreate()"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["'/remote_home/projects2/dsor-651-examples/examples/data/Lottery_Powerball_Winning_Numbers__Beginning_2010.csv'"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","\n","data_file_path = os.path.abspath(\"../data/Lottery_Powerball_Winning_Numbers__Beginning_2010.csv\")\n","data_file_path"]},{"cell_type":"code","execution_count":3,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f94a90bf-017e-4549-b278-0c1e31314be2","showTitle":false,"title":""}},"outputs":[],"source":["df1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(data_file_path)"]},{"cell_type":"code","execution_count":4,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"3db29762-24bf-4c57-be9a-3740b1b1c4ee","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["['Multiplier',\n"," '__class__',\n"," '__delattr__',\n"," '__dict__',\n"," '__dir__',\n"," '__doc__',\n"," '__eq__',\n"," '__format__',\n"," '__ge__',\n"," '__getattr__',\n"," '__getattribute__',\n"," '__getitem__',\n"," '__gt__',\n"," '__hash__',\n"," '__init__',\n"," '__init_subclass__',\n"," '__le__',\n"," '__lt__',\n"," '__module__',\n"," '__ne__',\n"," '__new__',\n"," '__reduce__',\n"," '__reduce_ex__',\n"," '__repr__',\n"," '__setattr__',\n"," '__sizeof__',\n"," '__str__',\n"," '__subclasshook__',\n"," '__weakref__',\n"," '_collect_as_arrow',\n"," '_ipython_key_completions_',\n"," '_jcols',\n"," '_jdf',\n"," '_jmap',\n"," '_joinAsOf',\n"," '_jseq',\n"," '_lazy_rdd',\n"," '_repr_html_',\n"," '_sc',\n"," '_schema',\n"," '_session',\n"," '_show_string',\n"," '_sort_cols',\n"," '_sql_ctx',\n"," '_support_repr_html',\n"," 'agg',\n"," 'alias',\n"," 'approxQuantile',\n"," 'cache',\n"," 'checkpoint',\n"," 'coalesce',\n"," 'colRegex',\n"," 'collect',\n"," 'columns',\n"," 'corr',\n"," 'count',\n"," 'cov',\n"," 'createGlobalTempView',\n"," 'createOrReplaceGlobalTempView',\n"," 'createOrReplaceTempView',\n"," 'createTempView',\n"," 'crossJoin',\n"," 'crosstab',\n"," 'cube',\n"," 'describe',\n"," 'distinct',\n"," 'drop',\n"," 'dropDuplicates',\n"," 'dropDuplicatesWithinWatermark',\n"," 'drop_duplicates',\n"," 'dropna',\n"," 'dtypes',\n"," 'exceptAll',\n"," 'explain',\n"," 'fillna',\n"," 'filter',\n"," 'first',\n"," 'foreach',\n"," 'foreachPartition',\n"," 'freqItems',\n"," 'groupBy',\n"," 'groupby',\n"," 'head',\n"," 'hint',\n"," 'inputFiles',\n"," 'intersect',\n"," 'intersectAll',\n"," 'isEmpty',\n"," 'isLocal',\n"," 'isStreaming',\n"," 'is_cached',\n"," 'join',\n"," 'limit',\n"," 'localCheckpoint',\n"," 'mapInArrow',\n"," 'mapInPandas',\n"," 'melt',\n"," 'na',\n"," 'observe',\n"," 'offset',\n"," 'orderBy',\n"," 'pandas_api',\n"," 'persist',\n"," 'printSchema',\n"," 'randomSplit',\n"," 'rdd',\n"," 'registerTempTable',\n"," 'repartition',\n"," 'repartitionByRange',\n"," 'replace',\n"," 'rollup',\n"," 'sameSemantics',\n"," 'sample',\n"," 'sampleBy',\n"," 'schema',\n"," 'select',\n"," 'selectExpr',\n"," 'semanticHash',\n"," 'show',\n"," 'sort',\n"," 'sortWithinPartitions',\n"," 'sparkSession',\n"," 'sql_ctx',\n"," 'stat',\n"," 'storageLevel',\n"," 'subtract',\n"," 'summary',\n"," 'tail',\n"," 'take',\n"," 'to',\n"," 'toDF',\n"," 'toJSON',\n"," 'toLocalIterator',\n"," 'toPandas',\n"," 'to_koalas',\n"," 'to_pandas_on_spark',\n"," 'transform',\n"," 'union',\n"," 'unionAll',\n"," 'unionByName',\n"," 'unpersist',\n"," 'unpivot',\n"," 'where',\n"," 'withColumn',\n"," 'withColumnRenamed',\n"," 'withColumns',\n"," 'withColumnsRenamed',\n"," 'withMetadata',\n"," 'withWatermark',\n"," 'write',\n"," 'writeStream',\n"," 'writeTo']"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["dir(df1)"]},{"cell_type":"code","execution_count":5,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"8499f7b2-3c87-4ab9-aff5-2cdaf63b4e50","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["pyspark.sql.dataframe.DataFrame"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["type(df1)"]},{"cell_type":"code","execution_count":6,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"84d960e5-3848-44e5-aafc-46ddd525841a","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["pyspark.rdd.RDD"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["type(df1.rdd)"]},{"cell_type":"code","execution_count":7,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"df427a30-f33b-4a0d-ae7e-0b96e2f56eab","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["b'(1) MapPartitionsRDD[14] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[13] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  SQLExecutionRDD[12] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[11] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  FileScanRDD[10] at javaToPython at NativeMethodAccessorImpl.java:0 []'"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["df1.rdd.toDebugString()"]},{"cell_type":"code","execution_count":8,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"ec065187-7a2b-4a0d-9db7-750ba02b28f0","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["1"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["df1.rdd.getNumPartitions()"]},{"cell_type":"code","execution_count":9,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"2dda9800-d1b3-4e7c-9fa2-47c6fb7a45e0","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["pyspark.sql.session.SparkSession"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["type(spark)"]},{"cell_type":"code","execution_count":10,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"c56065e0-15ed-426f-9d31-437e8846de36","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["['Builder',\n"," '__annotations__',\n"," '__class__',\n"," '__delattr__',\n"," '__dict__',\n"," '__dir__',\n"," '__doc__',\n"," '__enter__',\n"," '__eq__',\n"," '__exit__',\n"," '__format__',\n"," '__ge__',\n"," '__getattribute__',\n"," '__gt__',\n"," '__hash__',\n"," '__init__',\n"," '__init_subclass__',\n"," '__le__',\n"," '__lt__',\n"," '__module__',\n"," '__ne__',\n"," '__new__',\n"," '__reduce__',\n"," '__reduce_ex__',\n"," '__repr__',\n"," '__setattr__',\n"," '__sizeof__',\n"," '__str__',\n"," '__subclasshook__',\n"," '__weakref__',\n"," '_activeSession',\n"," '_convert_from_pandas',\n"," '_createFromLocal',\n"," '_createFromRDD',\n"," '_create_dataframe',\n"," '_create_from_pandas_with_arrow',\n"," '_create_shell_session',\n"," '_getActiveSessionOrCreate',\n"," '_get_numpy_record_dtype',\n"," '_inferSchema',\n"," '_inferSchemaFromList',\n"," '_instantiatedSession',\n"," '_jconf',\n"," '_jsc',\n"," '_jsparkSession',\n"," '_jvm',\n"," '_repr_html_',\n"," '_sc',\n"," 'active',\n"," 'addArtifact',\n"," 'addArtifacts',\n"," 'addTag',\n"," 'builder',\n"," 'catalog',\n"," 'clearTags',\n"," 'client',\n"," 'conf',\n"," 'copyFromLocalToFs',\n"," 'createDataFrame',\n"," 'getActiveSession',\n"," 'getTags',\n"," 'interruptAll',\n"," 'interruptOperation',\n"," 'interruptTag',\n"," 'newSession',\n"," 'range',\n"," 'read',\n"," 'readStream',\n"," 'removeTag',\n"," 'sparkContext',\n"," 'sql',\n"," 'stop',\n"," 'streams',\n"," 'table',\n"," 'udf',\n"," 'udtf',\n"," 'version']"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["dir(spark)"]},{"cell_type":"code","execution_count":11,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"948008ad-703d-455a-afb0-197bd689b1cc","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["Help on SparkSession in module pyspark.sql.session object:\n","\n","class SparkSession(pyspark.sql.pandas.conversion.SparkConversionMixin)\n"," |  SparkSession(sparkContext: pyspark.context.SparkContext, jsparkSession: Optional[py4j.java_gateway.JavaObject] = None, options: Dict[str, Any] = {})\n"," |  \n"," |  The entry point to programming Spark with the Dataset and DataFrame API.\n"," |  \n"," |  A SparkSession can be used to create :class:`DataFrame`, register :class:`DataFrame` as\n"," |  tables, execute SQL over tables, cache tables, and read parquet files.\n"," |  To create a :class:`SparkSession`, use the following builder pattern:\n"," |  \n"," |  .. versionchanged:: 3.4.0\n"," |      Supports Spark Connect.\n"," |  \n"," |  .. autoattribute:: builder\n"," |     :annotation:\n"," |  \n"," |  Examples\n"," |  --------\n"," |  Create a Spark session.\n"," |  \n"," |  >>> spark = (\n"," |  ...     SparkSession.builder\n"," |  ...         .master(\"local\")\n"," |  ...         .appName(\"Word Count\")\n"," |  ...         .config(\"spark.some.config.option\", \"some-value\")\n"," |  ...         .getOrCreate()\n"," |  ... )\n"," |  \n"," |  Create a Spark session with Spark Connect.\n"," |  \n"," |  >>> spark = (\n"," |  ...     SparkSession.builder\n"," |  ...         .remote(\"sc://localhost\")\n"," |  ...         .appName(\"Word Count\")\n"," |  ...         .config(\"spark.some.config.option\", \"some-value\")\n"," |  ...         .getOrCreate()\n"," |  ... )  # doctest: +SKIP\n"," |  \n"," |  Method resolution order:\n"," |      SparkSession\n"," |      pyspark.sql.pandas.conversion.SparkConversionMixin\n"," |      builtins.object\n"," |  \n"," |  Methods defined here:\n"," |  \n"," |  __enter__(self) -> 'SparkSession'\n"," |      Enable 'with SparkSession.builder.(...).getOrCreate() as session: app' syntax.\n"," |      \n"," |      .. versionadded:: 2.0.0\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> with SparkSession.builder.master(\"local\").getOrCreate() as session:\n"," |      ...     session.range(5).show()  # doctest: +SKIP\n"," |      +---+\n"," |      | id|\n"," |      +---+\n"," |      |  0|\n"," |      |  1|\n"," |      |  2|\n"," |      |  3|\n"," |      |  4|\n"," |      +---+\n"," |  \n"," |  __exit__(self, exc_type: Optional[Type[BaseException]], exc_val: Optional[BaseException], exc_tb: Optional[traceback]) -> None\n"," |      Enable 'with SparkSession.builder.(...).getOrCreate() as session: app' syntax.\n"," |      \n"," |      Specifically stop the SparkSession on exit of the with block.\n"," |      \n"," |      .. versionadded:: 2.0.0\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> with SparkSession.builder.master(\"local\").getOrCreate() as session:\n"," |      ...     session.range(5).show()  # doctest: +SKIP\n"," |      +---+\n"," |      | id|\n"," |      +---+\n"," |      |  0|\n"," |      |  1|\n"," |      |  2|\n"," |      |  3|\n"," |      |  4|\n"," |      +---+\n"," |  \n"," |  __init__(self, sparkContext: pyspark.context.SparkContext, jsparkSession: Optional[py4j.java_gateway.JavaObject] = None, options: Dict[str, Any] = {})\n"," |      Initialize self.  See help(type(self)) for accurate signature.\n"," |  \n"," |  addArtifact = addArtifacts(self, *path: str, pyfile: bool = False, archive: bool = False, file: bool = False) -> None\n"," |  \n"," |  addArtifacts(self, *path: str, pyfile: bool = False, archive: bool = False, file: bool = False) -> None\n"," |      Add artifact(s) to the client session. Currently only local files are supported.\n"," |      \n"," |      .. versionadded:: 3.5.0\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      *path : tuple of str\n"," |          Artifact's URIs to add.\n"," |      pyfile : bool\n"," |          Whether to add them as Python dependencies such as .py, .egg, .zip or .jar files.\n"," |          The pyfiles are directly inserted into the path when executing Python functions\n"," |          in executors.\n"," |      archive : bool\n"," |          Whether to add them as archives such as .zip, .jar, .tar.gz, .tgz, or .tar files.\n"," |          The archives are unpacked on the executor side automatically.\n"," |      file : bool\n"," |          Add a file to be downloaded with this Spark job on every node.\n"," |          The ``path`` passed can only be a local file for now.\n"," |      \n"," |      Notes\n"," |      -----\n"," |      This is an API dedicated to Spark Connect client only. With regular Spark Session, it throws\n"," |      an exception.\n"," |  \n"," |  addTag(self, tag: str) -> None\n"," |      Add a tag to be assigned to all the operations started by this thread in this session.\n"," |      \n"," |      .. versionadded:: 3.5.0\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      tag : list of str\n"," |          The tag to be added. Cannot contain ',' (comma) character or be an empty string.\n"," |  \n"," |  clearTags(self) -> None\n"," |      Clear the current thread's operation tags.\n"," |      \n"," |      .. versionadded:: 3.5.0\n"," |  \n"," |  copyFromLocalToFs(self, local_path: str, dest_path: str) -> None\n"," |      Copy file from local to cloud storage file system.\n"," |      If the file already exits in destination path, old file is overwritten.\n"," |      \n"," |      .. versionadded:: 3.5.0\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      local_path: str\n"," |          Path to a local file. Directories are not supported.\n"," |          The path can be either an absolute path or a relative path.\n"," |      dest_path: str\n"," |          The cloud storage path to the destination the file will\n"," |          be copied to.\n"," |          The path must be an an absolute path.\n"," |      \n"," |      Notes\n"," |      -----\n"," |      This API is a developer API.\n"," |      Also, this is an API dedicated to Spark Connect client only. With regular\n"," |      Spark Session, it throws an exception.\n"," |  \n"," |  createDataFrame(self, data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike'), ForwardRef('ArrayLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Optional[float] = None, verifySchema: bool = True) -> pyspark.sql.dataframe.DataFrame\n"," |      Creates a :class:`DataFrame` from an :class:`RDD`, a list, a :class:`pandas.DataFrame`\n"," |      or a :class:`numpy.ndarray`.\n"," |      \n"," |      .. versionadded:: 2.0.0\n"," |      \n"," |      .. versionchanged:: 3.4.0\n"," |          Supports Spark Connect.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      data : :class:`RDD` or iterable\n"," |          an RDD of any kind of SQL data representation (:class:`Row`,\n"," |          :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`,\n"," |          :class:`pandas.DataFrame` or :class:`numpy.ndarray`.\n"," |      schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n"," |          a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n"," |          column names, default is None. The data type string format equals to\n"," |          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n"," |          omit the ``struct<>``.\n"," |      \n"," |          When ``schema`` is a list of column names, the type of each column\n"," |          will be inferred from ``data``.\n"," |      \n"," |          When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n"," |          from ``data``, which should be an RDD of either :class:`Row`,\n"," |          :class:`namedtuple`, or :class:`dict`.\n"," |      \n"," |          When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must\n"," |          match the real data, or an exception will be thrown at runtime. If the given schema is\n"," |          not :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n"," |          :class:`pyspark.sql.types.StructType` as its only field, and the field name will be\n"," |          \"value\". Each record will also be wrapped into a tuple, which can be converted to row\n"," |          later.\n"," |      samplingRatio : float, optional\n"," |          the sample ratio of rows used for inferring. The first few rows will be used\n"," |          if ``samplingRatio`` is ``None``.\n"," |      verifySchema : bool, optional\n"," |          verify data types of every row against schema. Enabled by default.\n"," |      \n"," |          .. versionadded:: 2.1.0\n"," |      \n"," |      Returns\n"," |      -------\n"," |      :class:`DataFrame`\n"," |      \n"," |      Notes\n"," |      -----\n"," |      Usage with `spark.sql.execution.arrow.pyspark.enabled=True` is experimental.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      Create a DataFrame from a list of tuples.\n"," |      \n"," |      >>> spark.createDataFrame([('Alice', 1)]).show()\n"," |      +-----+---+\n"," |      |   _1| _2|\n"," |      +-----+---+\n"," |      |Alice|  1|\n"," |      +-----+---+\n"," |      \n"," |      Create a DataFrame from a list of dictionaries.\n"," |      \n"," |      >>> d = [{'name': 'Alice', 'age': 1}]\n"," |      >>> spark.createDataFrame(d).show()\n"," |      +---+-----+\n"," |      |age| name|\n"," |      +---+-----+\n"," |      |  1|Alice|\n"," |      +---+-----+\n"," |      \n"," |      Create a DataFrame with column names specified.\n"," |      \n"," |      >>> spark.createDataFrame([('Alice', 1)], ['name', 'age']).show()\n"," |      +-----+---+\n"," |      | name|age|\n"," |      +-----+---+\n"," |      |Alice|  1|\n"," |      +-----+---+\n"," |      \n"," |      Create a DataFrame with the explicit schema specified.\n"," |      \n"," |      >>> from pyspark.sql.types import *\n"," |      >>> schema = StructType([\n"," |      ...    StructField(\"name\", StringType(), True),\n"," |      ...    StructField(\"age\", IntegerType(), True)])\n"," |      >>> spark.createDataFrame([('Alice', 1)], schema).show()\n"," |      +-----+---+\n"," |      | name|age|\n"," |      +-----+---+\n"," |      |Alice|  1|\n"," |      +-----+---+\n"," |      \n"," |      Create a DataFrame with the schema in DDL formatted string.\n"," |      \n"," |      >>> spark.createDataFrame([('Alice', 1)], \"name: string, age: int\").show()\n"," |      +-----+---+\n"," |      | name|age|\n"," |      +-----+---+\n"," |      |Alice|  1|\n"," |      +-----+---+\n"," |      \n"," |      Create an empty DataFrame.\n"," |      When initializing an empty DataFrame in PySpark, it's mandatory to specify its schema,\n"," |      as the DataFrame lacks data from which the schema can be inferred.\n"," |      \n"," |      >>> spark.createDataFrame([], \"name: string, age: int\").show()\n"," |      +----+---+\n"," |      |name|age|\n"," |      +----+---+\n"," |      +----+---+\n"," |      \n"," |      Create a DataFrame from Row objects.\n"," |      \n"," |      >>> from pyspark.sql import Row\n"," |      >>> Person = Row('name', 'age')\n"," |      >>> df = spark.createDataFrame([Person(\"Alice\", 1)])\n"," |      >>> df.show()\n"," |      +-----+---+\n"," |      | name|age|\n"," |      +-----+---+\n"," |      |Alice|  1|\n"," |      +-----+---+\n"," |      \n"," |      Create a DataFrame from a pandas DataFrame.\n"," |      \n"," |      >>> spark.createDataFrame(df.toPandas()).show()  # doctest: +SKIP\n"," |      +-----+---+\n"," |      | name|age|\n"," |      +-----+---+\n"," |      |Alice|  1|\n"," |      +-----+---+\n"," |      >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n"," |      +---+---+\n"," |      |  0|  1|\n"," |      +---+---+\n"," |      |  1|  2|\n"," |      +---+---+\n"," |  \n"," |  getTags(self) -> Set[str]\n"," |      Get the tags that are currently set to be assigned to all the operations started by this\n"," |      thread.\n"," |      \n"," |      .. versionadded:: 3.5.0\n"," |      \n"," |      Returns\n"," |      -------\n"," |      set of str\n"," |          Set of tags of interrupted operations.\n"," |  \n"," |  interruptAll(self) -> List[str]\n"," |      Interrupt all operations of this session currently running on the connected server.\n"," |      \n"," |      .. versionadded:: 3.5.0\n"," |      \n"," |      Returns\n"," |      -------\n"," |      list of str\n"," |          List of operationIds of interrupted operations.\n"," |      \n"," |      Notes\n"," |      -----\n"," |      There is still a possibility of operation finishing just as it is interrupted.\n"," |  \n"," |  interruptOperation(self, op_id: str) -> List[str]\n"," |      Interrupt an operation of this session with the given operationId.\n"," |      \n"," |      .. versionadded:: 3.5.0\n"," |      \n"," |      Returns\n"," |      -------\n"," |      list of str\n"," |          List of operationIds of interrupted operations.\n"," |      \n"," |      Notes\n"," |      -----\n"," |      There is still a possibility of operation finishing just as it is interrupted.\n"," |  \n"," |  interruptTag(self, tag: str) -> List[str]\n"," |      Interrupt all operations of this session with the given operation tag.\n"," |      \n"," |      .. versionadded:: 3.5.0\n"," |      \n"," |      Returns\n"," |      -------\n"," |      list of str\n"," |          List of operationIds of interrupted operations.\n"," |      \n"," |      Notes\n"," |      -----\n"," |      There is still a possibility of operation finishing just as it is interrupted.\n"," |  \n"," |  newSession(self) -> 'SparkSession'\n"," |      Returns a new :class:`SparkSession` as new session, that has separate SQLConf,\n"," |      registered temporary views and UDFs, but shared :class:`SparkContext` and\n"," |      table cache.\n"," |      \n"," |      .. versionadded:: 2.0.0\n"," |      \n"," |      Returns\n"," |      -------\n"," |      :class:`SparkSession`\n"," |          Spark session if an active session exists for the current thread\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> spark.newSession()\n"," |      <...SparkSession object ...>\n"," |  \n"," |  range(self, start: int, end: Optional[int] = None, step: int = 1, numPartitions: Optional[int] = None) -> pyspark.sql.dataframe.DataFrame\n"," |      Create a :class:`DataFrame` with single :class:`pyspark.sql.types.LongType` column named\n"," |      ``id``, containing elements in a range from ``start`` to ``end`` (exclusive) with\n"," |      step value ``step``.\n"," |      \n"," |      .. versionadded:: 2.0.0\n"," |      \n"," |      .. versionchanged:: 3.4.0\n"," |          Supports Spark Connect.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      start : int\n"," |          the start value\n"," |      end : int, optional\n"," |          the end value (exclusive)\n"," |      step : int, optional\n"," |          the incremental step (default: 1)\n"," |      numPartitions : int, optional\n"," |          the number of partitions of the DataFrame\n"," |      \n"," |      Returns\n"," |      -------\n"," |      :class:`DataFrame`\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> spark.range(1, 7, 2).show()\n"," |      +---+\n"," |      | id|\n"," |      +---+\n"," |      |  1|\n"," |      |  3|\n"," |      |  5|\n"," |      +---+\n"," |      \n"," |      If only one argument is specified, it will be used as the end value.\n"," |      \n"," |      >>> spark.range(3).show()\n"," |      +---+\n"," |      | id|\n"," |      +---+\n"," |      |  0|\n"," |      |  1|\n"," |      |  2|\n"," |      +---+\n"," |  \n"," |  removeTag(self, tag: str) -> None\n"," |      Remove a tag previously added to be assigned to all the operations started by this thread in\n"," |      this session. Noop if such a tag was not added earlier.\n"," |      \n"," |      .. versionadded:: 3.5.0\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      tag : list of str\n"," |          The tag to be removed. Cannot contain ',' (comma) character or be an empty string.\n"," |  \n"," |  sql(self, sqlQuery: str, args: Union[Dict[str, Any], List, NoneType] = None, **kwargs: Any) -> pyspark.sql.dataframe.DataFrame\n"," |      Returns a :class:`DataFrame` representing the result of the given query.\n"," |      When ``kwargs`` is specified, this method formats the given string by using the Python\n"," |      standard formatter. The method binds named parameters to SQL literals or\n"," |      positional parameters from `args`. It doesn't support named and positional parameters\n"," |      in the same SQL query.\n"," |      \n"," |      .. versionadded:: 2.0.0\n"," |      \n"," |      .. versionchanged:: 3.4.0\n"," |          Supports Spark Connect and parameterized SQL.\n"," |      \n"," |      .. versionchanged:: 3.5.0\n"," |          Added positional parameters.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      sqlQuery : str\n"," |          SQL query string.\n"," |      args : dict or list\n"," |          A dictionary of parameter names to Python objects or a list of Python objects\n"," |          that can be converted to SQL literal expressions. See\n"," |          <a href=\"https://spark.apache.org/docs/latest/sql-ref-datatypes.html\">\n"," |          Supported Data Types</a> for supported value types in Python.\n"," |          For example, dictionary keys: \"rank\", \"name\", \"birthdate\";\n"," |          dictionary or list values: 1, \"Steven\", datetime.date(2023, 4, 2).\n"," |          A value can be also a `Column` of literal expression, in that case it is taken as is.\n"," |      \n"," |          .. versionadded:: 3.4.0\n"," |      \n"," |      kwargs : dict\n"," |          Other variables that the user wants to set that can be referenced in the query\n"," |      \n"," |          .. versionchanged:: 3.3.0\n"," |             Added optional argument ``kwargs`` to specify the mapping of variables in the query.\n"," |             This feature is experimental and unstable.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      :class:`DataFrame`\n"," |      \n"," |      Examples\n"," |      --------\n"," |      Executing a SQL query.\n"," |      \n"," |      >>> spark.sql(\"SELECT * FROM range(10) where id > 7\").show()\n"," |      +---+\n"," |      | id|\n"," |      +---+\n"," |      |  8|\n"," |      |  9|\n"," |      +---+\n"," |      \n"," |      Executing a SQL query with variables as Python formatter standard.\n"," |      \n"," |      >>> spark.sql(\n"," |      ...     \"SELECT * FROM range(10) WHERE id > {bound1} AND id < {bound2}\", bound1=7, bound2=9\n"," |      ... ).show()\n"," |      +---+\n"," |      | id|\n"," |      +---+\n"," |      |  8|\n"," |      +---+\n"," |      \n"," |      >>> mydf = spark.range(10)\n"," |      >>> spark.sql(\n"," |      ...     \"SELECT {col} FROM {mydf} WHERE id IN {x}\",\n"," |      ...     col=mydf.id, mydf=mydf, x=tuple(range(4))).show()\n"," |      +---+\n"," |      | id|\n"," |      +---+\n"," |      |  0|\n"," |      |  1|\n"," |      |  2|\n"," |      |  3|\n"," |      +---+\n"," |      \n"," |      >>> spark.sql('''\n"," |      ...   SELECT m1.a, m2.b\n"," |      ...   FROM {table1} m1 INNER JOIN {table2} m2\n"," |      ...   ON m1.key = m2.key\n"," |      ...   ORDER BY m1.a, m2.b''',\n"," |      ...   table1=spark.createDataFrame([(1, \"a\"), (2, \"b\")], [\"a\", \"key\"]),\n"," |      ...   table2=spark.createDataFrame([(3, \"a\"), (4, \"b\"), (5, \"b\")], [\"b\", \"key\"])).show()\n"," |      +---+---+\n"," |      |  a|  b|\n"," |      +---+---+\n"," |      |  1|  3|\n"," |      |  2|  4|\n"," |      |  2|  5|\n"," |      +---+---+\n"," |      \n"," |      Also, it is possible to query using class:`Column` from :class:`DataFrame`.\n"," |      \n"," |      >>> mydf = spark.createDataFrame([(1, 4), (2, 4), (3, 6)], [\"A\", \"B\"])\n"," |      >>> spark.sql(\"SELECT {df.A}, {df[B]} FROM {df}\", df=mydf).show()\n"," |      +---+---+\n"," |      |  A|  B|\n"," |      +---+---+\n"," |      |  1|  4|\n"," |      |  2|  4|\n"," |      |  3|  6|\n"," |      +---+---+\n"," |      \n"," |      And substitude named parameters with the `:` prefix by SQL literals.\n"," |      \n"," |      >>> spark.sql(\"SELECT * FROM {df} WHERE {df[B]} > :minB\", {\"minB\" : 5}, df=mydf).show()\n"," |      +---+---+\n"," |      |  A|  B|\n"," |      +---+---+\n"," |      |  3|  6|\n"," |      +---+---+\n"," |      \n"," |      Or positional parameters marked by `?` in the SQL query by SQL literals.\n"," |      \n"," |      >>> spark.sql(\n"," |      ...   \"SELECT * FROM {df} WHERE {df[B]} > ? and ? < {df[A]}\",\n"," |      ...   args=[5, 2], df=mydf).show()\n"," |      +---+---+\n"," |      |  A|  B|\n"," |      +---+---+\n"," |      |  3|  6|\n"," |      +---+---+\n"," |  \n"," |  stop(self) -> None\n"," |      Stop the underlying :class:`SparkContext`.\n"," |      \n"," |      .. versionadded:: 2.0.0\n"," |      \n"," |      .. versionchanged:: 3.4.0\n"," |          Supports Spark Connect.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> spark.stop()  # doctest: +SKIP\n"," |  \n"," |  table(self, tableName: str) -> pyspark.sql.dataframe.DataFrame\n"," |      Returns the specified table as a :class:`DataFrame`.\n"," |      \n"," |      .. versionadded:: 2.0.0\n"," |      \n"," |      .. versionchanged:: 3.4.0\n"," |          Supports Spark Connect.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      tableName : str\n"," |          the table name to retrieve.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      :class:`DataFrame`\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> spark.range(5).createOrReplaceTempView(\"table1\")\n"," |      >>> spark.table(\"table1\").sort(\"id\").show()\n"," |      +---+\n"," |      | id|\n"," |      +---+\n"," |      |  0|\n"," |      |  1|\n"," |      |  2|\n"," |      |  3|\n"," |      |  4|\n"," |      +---+\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Class methods defined here:\n"," |  \n"," |  active() -> 'SparkSession' from builtins.type\n"," |      Returns the active or default :class:`SparkSession` for the current thread, returned by\n"," |      the builder.\n"," |      \n"," |      .. versionadded:: 3.5.0\n"," |      \n"," |      Returns\n"," |      -------\n"," |      :class:`SparkSession`\n"," |          Spark session if an active or default session exists for the current thread.\n"," |  \n"," |  getActiveSession() -> Optional[ForwardRef('SparkSession')] from builtins.type\n"," |      Returns the active :class:`SparkSession` for the current thread, returned by the builder\n"," |      \n"," |      .. versionadded:: 3.0.0\n"," |      \n"," |      .. versionchanged:: 3.5.0\n"," |          Supports Spark Connect.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      :class:`SparkSession`\n"," |          Spark session if an active session exists for the current thread\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> s = SparkSession.getActiveSession()\n"," |      >>> df = s.createDataFrame([('Alice', 1)], ['name', 'age'])\n"," |      >>> df.select(\"age\").show()\n"," |      +---+\n"," |      |age|\n"," |      +---+\n"," |      |  1|\n"," |      +---+\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Readonly properties defined here:\n"," |  \n"," |  builder\n"," |      Creates a :class:`Builder` for constructing a :class:`SparkSession`.\n"," |      \n"," |      .. versionchanged:: 3.4.0\n"," |          Supports Spark Connect.\n"," |  \n"," |  catalog\n"," |      Interface through which the user may create, drop, alter or query underlying\n"," |      databases, tables, functions, etc.\n"," |      \n"," |      .. versionadded:: 2.0.0\n"," |      \n"," |      .. versionchanged:: 3.4.0\n"," |          Supports Spark Connect.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      :class:`Catalog`\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> spark.catalog\n"," |      <...Catalog object ...>\n"," |      \n"," |      Create a temp view, show the list, and drop it.\n"," |      \n"," |      >>> spark.range(1).createTempView(\"test_view\")\n"," |      >>> spark.catalog.listTables()\n"," |      [Table(name='test_view', catalog=None, namespace=[], description=None, ...\n"," |      >>> _ = spark.catalog.dropTempView(\"test_view\")\n"," |  \n"," |  client\n"," |      Gives access to the Spark Connect client. In normal cases this is not necessary to be used\n"," |      and only relevant for testing.\n"," |      \n"," |      .. versionadded:: 3.4.0\n"," |      \n"," |      Returns\n"," |      -------\n"," |      :class:`SparkConnectClient`\n"," |      \n"," |      Notes\n"," |      -----\n"," |      This API is unstable, and a developer API. It returns non-API instance\n"," |      :class:`SparkConnectClient`.\n"," |      This is an API dedicated to Spark Connect client only. With regular Spark Session, it throws\n"," |      an exception.\n"," |  \n"," |  conf\n"," |      Runtime configuration interface for Spark.\n"," |      \n"," |      This is the interface through which the user can get and set all Spark and Hadoop\n"," |      configurations that are relevant to Spark SQL. When getting the value of a config,\n"," |      this defaults to the value set in the underlying :class:`SparkContext`, if any.\n"," |      \n"," |      .. versionadded:: 2.0.0\n"," |      \n"," |      .. versionchanged:: 3.4.0\n"," |          Supports Spark Connect.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      :class:`pyspark.sql.conf.RuntimeConfig`\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> spark.conf\n"," |      <pyspark...RuntimeConf...>\n"," |      \n"," |      Set a runtime configuration for the session\n"," |      \n"," |      >>> spark.conf.set(\"key\", \"value\")\n"," |      >>> spark.conf.get(\"key\")\n"," |      'value'\n"," |  \n"," |  read\n"," |      Returns a :class:`DataFrameReader` that can be used to read data\n"," |      in as a :class:`DataFrame`.\n"," |      \n"," |      .. versionadded:: 2.0.0\n"," |      \n"," |      .. versionchanged:: 3.4.0\n"," |          Supports Spark Connect.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      :class:`DataFrameReader`\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> spark.read\n"," |      <...DataFrameReader object ...>\n"," |      \n"," |      Write a DataFrame into a JSON file and read it back.\n"," |      \n"," |      >>> import tempfile\n"," |      >>> with tempfile.TemporaryDirectory() as d:\n"," |      ...     # Write a DataFrame into a JSON file\n"," |      ...     spark.createDataFrame(\n"," |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n"," |      ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n"," |      ...\n"," |      ...     # Read the JSON file as a DataFrame.\n"," |      ...     spark.read.format('json').load(d).show()\n"," |      +---+------------+\n"," |      |age|        name|\n"," |      +---+------------+\n"," |      |100|Hyukjin Kwon|\n"," |      +---+------------+\n"," |  \n"," |  readStream\n"," |      Returns a :class:`DataStreamReader` that can be used to read data streams\n"," |      as a streaming :class:`DataFrame`.\n"," |      \n"," |      .. versionadded:: 2.0.0\n"," |      \n"," |      .. versionchanged:: 3.5.0\n"," |          Supports Spark Connect.\n"," |      \n"," |      Notes\n"," |      -----\n"," |      This API is evolving.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      :class:`DataStreamReader`\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> spark.readStream\n"," |      <pyspark...DataStreamReader object ...>\n"," |      \n"," |      The example below uses Rate source that generates rows continuously.\n"," |      After that, we operate a modulo by 3, and then write the stream out to the console.\n"," |      The streaming query stops in 3 seconds.\n"," |      \n"," |      >>> import time\n"," |      >>> df = spark.readStream.format(\"rate\").load()\n"," |      >>> df = df.selectExpr(\"value % 3 as v\")\n"," |      >>> q = df.writeStream.format(\"console\").start()\n"," |      >>> time.sleep(3)\n"," |      >>> q.stop()\n"," |  \n"," |  sparkContext\n"," |      Returns the underlying :class:`SparkContext`.\n"," |      \n"," |      .. versionadded:: 2.0.0\n"," |      \n"," |      Returns\n"," |      -------\n"," |      :class:`SparkContext`\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> spark.sparkContext\n"," |      <SparkContext master=... appName=...>\n"," |      \n"," |      Create an RDD from the Spark context\n"," |      \n"," |      >>> rdd = spark.sparkContext.parallelize([1, 2, 3])\n"," |      >>> rdd.collect()\n"," |      [1, 2, 3]\n"," |  \n"," |  streams\n"," |      Returns a :class:`StreamingQueryManager` that allows managing all the\n"," |      :class:`StreamingQuery` instances active on `this` context.\n"," |      \n"," |      .. versionadded:: 2.0.0\n"," |      \n"," |      .. versionchanged:: 3.5.0\n"," |          Supports Spark Connect.\n"," |      \n"," |      Notes\n"," |      -----\n"," |      This API is evolving.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      :class:`StreamingQueryManager`\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> spark.streams\n"," |      <pyspark...StreamingQueryManager object ...>\n"," |      \n"," |      Get the list of active streaming queries\n"," |      \n"," |      >>> sq = spark.readStream.format(\n"," |      ...     \"rate\").load().writeStream.format('memory').queryName('this_query').start()\n"," |      >>> sqm = spark.streams\n"," |      >>> [q.name for q in sqm.active]\n"," |      ['this_query']\n"," |      >>> sq.stop()\n"," |  \n"," |  udf\n"," |      Returns a :class:`UDFRegistration` for UDF registration.\n"," |      \n"," |      .. versionadded:: 2.0.0\n"," |      \n"," |      .. versionchanged:: 3.4.0\n"," |          Supports Spark Connect.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      :class:`UDFRegistration`\n"," |      \n"," |      Examples\n"," |      --------\n"," |      Register a Python UDF, and use it in SQL.\n"," |      \n"," |      >>> strlen = spark.udf.register(\"strlen\", lambda x: len(x))\n"," |      >>> spark.sql(\"SELECT strlen('test')\").show()\n"," |      +------------+\n"," |      |strlen(test)|\n"," |      +------------+\n"," |      |           4|\n"," |      +------------+\n"," |  \n"," |  udtf\n"," |      Returns a :class:`UDTFRegistration` for UDTF registration.\n"," |      \n"," |      .. versionadded:: 3.5.0\n"," |      \n"," |      Returns\n"," |      -------\n"," |      :class:`UDTFRegistration`\n"," |      \n"," |      Notes\n"," |      -----\n"," |      Supports Spark Connect.\n"," |  \n"," |  version\n"," |      The version of Spark on which this application is running.\n"," |      \n"," |      .. versionadded:: 2.0.0\n"," |      \n"," |      .. versionchanged:: 3.4.0\n"," |          Supports Spark Connect.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      str\n"," |          the version of Spark in string.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> _ = spark.version\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data and other attributes defined here:\n"," |  \n"," |  Builder = <class 'pyspark.sql.session.SparkSession.Builder'>\n"," |      Builder for :class:`SparkSession`.\n"," |  \n"," |  \n"," |  __annotations__ = {'_activeSession': typing.ClassVar[typing.Optional[F...\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors inherited from pyspark.sql.pandas.conversion.SparkConversionMixin:\n"," |  \n"," |  __dict__\n"," |      dictionary for instance variables (if defined)\n"," |  \n"," |  __weakref__\n"," |      list of weak references to the object (if defined)\n","\n"]}],"source":["help(spark)"]},{"cell_type":"code","execution_count":12,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"50429a5f-81f2-4b8a-a0cd-9af9948809d1","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["['PACKAGE_EXTENSIONS',\n"," '__annotations__',\n"," '__class__',\n"," '__delattr__',\n"," '__dict__',\n"," '__dir__',\n"," '__doc__',\n"," '__enter__',\n"," '__eq__',\n"," '__exit__',\n"," '__format__',\n"," '__ge__',\n"," '__getattribute__',\n"," '__getnewargs__',\n"," '__gt__',\n"," '__hash__',\n"," '__init__',\n"," '__init_subclass__',\n"," '__le__',\n"," '__lt__',\n"," '__module__',\n"," '__ne__',\n"," '__new__',\n"," '__reduce__',\n"," '__reduce_ex__',\n"," '__repr__',\n"," '__setattr__',\n"," '__sizeof__',\n"," '__str__',\n"," '__subclasshook__',\n"," '__weakref__',\n"," '_accumulatorServer',\n"," '_active_spark_context',\n"," '_assert_on_driver',\n"," '_batchSize',\n"," '_callsite',\n"," '_checkpointFile',\n"," '_conf',\n"," '_dictToJavaMap',\n"," '_do_init',\n"," '_encryption_enabled',\n"," '_ensure_initialized',\n"," '_gateway',\n"," '_getJavaStorageLevel',\n"," '_initialize_context',\n"," '_javaAccumulator',\n"," '_jsc',\n"," '_jvm',\n"," '_lock',\n"," '_next_accum_id',\n"," '_pickled_broadcast_vars',\n"," '_python_includes',\n"," '_repr_html_',\n"," '_serialize_to_jvm',\n"," '_temp_dir',\n"," '_unbatched_serializer',\n"," 'accumulator',\n"," 'addArchive',\n"," 'addFile',\n"," 'addJobTag',\n"," 'addPyFile',\n"," 'appName',\n"," 'applicationId',\n"," 'binaryFiles',\n"," 'binaryRecords',\n"," 'broadcast',\n"," 'cancelAllJobs',\n"," 'cancelJobGroup',\n"," 'cancelJobsWithTag',\n"," 'clearJobTags',\n"," 'defaultMinPartitions',\n"," 'defaultParallelism',\n"," 'dump_profiles',\n"," 'emptyRDD',\n"," 'environment',\n"," 'getCheckpointDir',\n"," 'getConf',\n"," 'getJobTags',\n"," 'getLocalProperty',\n"," 'getOrCreate',\n"," 'hadoopFile',\n"," 'hadoopRDD',\n"," 'listArchives',\n"," 'listFiles',\n"," 'master',\n"," 'newAPIHadoopFile',\n"," 'newAPIHadoopRDD',\n"," 'parallelize',\n"," 'pickleFile',\n"," 'profiler_collector',\n"," 'pythonExec',\n"," 'pythonVer',\n"," 'range',\n"," 'removeJobTag',\n"," 'resources',\n"," 'runJob',\n"," 'sequenceFile',\n"," 'serializer',\n"," 'setCheckpointDir',\n"," 'setInterruptOnCancel',\n"," 'setJobDescription',\n"," 'setJobGroup',\n"," 'setLocalProperty',\n"," 'setLogLevel',\n"," 'setSystemProperty',\n"," 'show_profiles',\n"," 'sparkHome',\n"," 'sparkUser',\n"," 'startTime',\n"," 'statusTracker',\n"," 'stop',\n"," 'textFile',\n"," 'uiWebUrl',\n"," 'union',\n"," 'version',\n"," 'wholeTextFiles']"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["dir(spark.sparkContext)"]},{"cell_type":"code","execution_count":13,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b1c2adec-5286-4b8a-81c9-fb0d0493fb4b","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["Help on method repartition in module pyspark.sql.dataframe:\n","\n","repartition(numPartitions: Union[int, ForwardRef('ColumnOrName')], *cols: 'ColumnOrName') -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n","    Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n","    resulting :class:`DataFrame` is hash partitioned.\n","    \n","    .. versionadded:: 1.3.0\n","    \n","    .. versionchanged:: 3.4.0\n","        Supports Spark Connect.\n","    \n","    Parameters\n","    ----------\n","    numPartitions : int\n","        can be an int to specify the target number of partitions or a Column.\n","        If it is a Column, it will be used as the first partitioning column. If not specified,\n","        the default number of partitions is used.\n","    cols : str or :class:`Column`\n","        partitioning columns.\n","    \n","        .. versionchanged:: 1.6.0\n","           Added optional arguments to specify the partitioning columns. Also made numPartitions\n","           optional if partitioning columns are specified.\n","    \n","    Returns\n","    -------\n","    :class:`DataFrame`\n","        Repartitioned DataFrame.\n","    \n","    Examples\n","    --------\n","    >>> df = spark.createDataFrame(\n","    ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n","    \n","    Repartition the data into 10 partitions.\n","    \n","    >>> df.repartition(10).rdd.getNumPartitions()\n","    10\n","    \n","    Repartition the data into 7 partitions by 'age' column.\n","    \n","    >>> df.repartition(7, \"age\").rdd.getNumPartitions()\n","    7\n","    \n","    Repartition the data into 7 partitions by 'age' and 'name columns.\n","    \n","    >>> df.repartition(3, \"name\", \"age\").rdd.getNumPartitions()\n","    3\n","\n"]}],"source":["help(df1.repartition)"]},{"cell_type":"code","execution_count":14,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"4b737a50-d8d5-4c59-ad0e-1c958e4cd9ec","showTitle":false,"title":""}},"outputs":[],"source":["df2 = df1.repartition(4)"]},{"cell_type":"code","execution_count":15,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"3967db16-86ed-4e42-9ffe-f24701eaf662","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["4"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["df2.rdd.getNumPartitions()"]},{"cell_type":"code","execution_count":16,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"a7d3457d-c1cb-4f8e-b8c6-7c15c35a2359","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["b'(4) MapPartitionsRDD[22] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[21] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  SQLExecutionRDD[20] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  ShuffledRowRDD[19] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n +-(1) MapPartitionsRDD[18] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n    |  MapPartitionsRDD[17] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n    |  MapPartitionsRDD[16] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n    |  FileScanRDD[15] at javaToPython at NativeMethodAccessorImpl.java:0 []'"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["df2.rdd.toDebugString()"]},{"cell_type":"code","execution_count":17,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b717d45a-314d-47d8-bd7a-d19cdede4293","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["1"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["df1.rdd.getNumPartitions()"]},{"cell_type":"code","execution_count":18,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"ed256c71-2750-425a-8de4-8ef0dd1abfc7","showTitle":false,"title":""}},"outputs":[],"source":["import timeit"]},{"cell_type":"code","execution_count":19,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"1059dccb-1bed-4175-a2e3-5a55d67b2ec1","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["['Draw Date', 'Winning Numbers', 'Multiplier']"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["df2.columns"]},{"cell_type":"code","execution_count":20,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"a0ef407f-1550-4b4e-a393-71ca0575252b","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["[Row(Draw Date='11/30/2022', Winning Numbers='04 19 24 47 66 10', Multiplier='2'),\n"," Row(Draw Date='09/12/2020', Winning Numbers='16 17 20 53 67 04', Multiplier='2'),\n"," Row(Draw Date='11/28/2015', Winning Numbers='02 06 47 66 67 02', Multiplier='3'),\n"," Row(Draw Date='12/09/2023', Winning Numbers='05 25 26 40 60 01', Multiplier='2'),\n"," Row(Draw Date='01/01/2022', Winning Numbers='06 12 39 48 50 07', Multiplier='2')]"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["df2.head(5)"]},{"cell_type":"code","execution_count":21,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"1e7a6261-732b-4176-9b29-967ac499ca78","showTitle":false,"title":""}},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+-------+----------+-----------------+------------------+\n","|summary| Draw Date|  Winning Numbers|        Multiplier|\n","+-------+----------+-----------------+------------------+\n","|  count|      1568|             1568|              1358|\n","|   mean|      NULL|             NULL| 2.803387334315169|\n","| stddev|      NULL|             NULL|1.1846800320678836|\n","|    min|01/01/2011|01 02 03 07 39 25|                10|\n","|    max|12/31/2022|52 58 59 64 66 09|                 5|\n","+-------+----------+-----------------+------------------+\n","\n","+-------+----------+-----------------+------------------+\n","|summary| Draw Date|  Winning Numbers|        Multiplier|\n","+-------+----------+-----------------+------------------+\n","|  count|      1568|             1568|              1358|\n","|   mean|      NULL|             NULL| 2.803387334315169|\n","| stddev|      NULL|             NULL|1.1846800320678836|\n","|    min|01/01/2011|01 02 03 07 39 25|                10|\n","|    max|12/31/2022|52 58 59 64 66 09|                 5|\n","+-------+----------+-----------------+------------------+\n","\n","+-------+----------+-----------------+------------------+\n","|summary| Draw Date|  Winning Numbers|        Multiplier|\n","+-------+----------+-----------------+------------------+\n","|  count|      1568|             1568|              1358|\n","|   mean|      NULL|             NULL| 2.803387334315169|\n","| stddev|      NULL|             NULL|1.1846800320678836|\n","|    min|01/01/2011|01 02 03 07 39 25|                10|\n","|    max|12/31/2022|52 58 59 64 66 09|                 5|\n","+-------+----------+-----------------+------------------+\n","\n","+-------+----------+-----------------+------------------+\n","|summary| Draw Date|  Winning Numbers|        Multiplier|\n","+-------+----------+-----------------+------------------+\n","|  count|      1568|             1568|              1358|\n","|   mean|      NULL|             NULL| 2.803387334315169|\n","| stddev|      NULL|             NULL|1.1846800320678836|\n","|    min|01/01/2011|01 02 03 07 39 25|                10|\n","|    max|12/31/2022|52 58 59 64 66 09|                 5|\n","+-------+----------+-----------------+------------------+\n","\n","+-------+----------+-----------------+------------------+\n","|summary| Draw Date|  Winning Numbers|        Multiplier|\n","+-------+----------+-----------------+------------------+\n","|  count|      1568|             1568|              1358|\n","|   mean|      NULL|             NULL| 2.803387334315169|\n","| stddev|      NULL|             NULL|1.1846800320678836|\n","|    min|01/01/2011|01 02 03 07 39 25|                10|\n","|    max|12/31/2022|52 58 59 64 66 09|                 5|\n","+-------+----------+-----------------+------------------+\n","\n"]},{"data":{"text/plain":["3.2594231236726046"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["timeit.timeit(lambda : df2.describe().show(),number=5)"]},{"cell_type":"code","execution_count":22,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"31578203-7bd7-425d-81b7-1405245c6947","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+----------+-----------------+------------------+\n","|summary| Draw Date|  Winning Numbers|        Multiplier|\n","+-------+----------+-----------------+------------------+\n","|  count|      1568|             1568|              1358|\n","|   mean|      NULL|             NULL| 2.803387334315169|\n","| stddev|      NULL|             NULL|1.1846800320678832|\n","|    min|01/01/2011|01 02 03 07 39 25|                10|\n","|    max|12/31/2022|52 58 59 64 66 09|                 5|\n","+-------+----------+-----------------+------------------+\n","\n","+-------+----------+-----------------+------------------+\n","|summary| Draw Date|  Winning Numbers|        Multiplier|\n","+-------+----------+-----------------+------------------+\n","|  count|      1568|             1568|              1358|\n","|   mean|      NULL|             NULL| 2.803387334315169|\n","| stddev|      NULL|             NULL|1.1846800320678832|\n","|    min|01/01/2011|01 02 03 07 39 25|                10|\n","|    max|12/31/2022|52 58 59 64 66 09|                 5|\n","+-------+----------+-----------------+------------------+\n","\n","+-------+----------+-----------------+------------------+\n","|summary| Draw Date|  Winning Numbers|        Multiplier|\n","+-------+----------+-----------------+------------------+\n","|  count|      1568|             1568|              1358|\n","|   mean|      NULL|             NULL| 2.803387334315169|\n","| stddev|      NULL|             NULL|1.1846800320678832|\n","|    min|01/01/2011|01 02 03 07 39 25|                10|\n","|    max|12/31/2022|52 58 59 64 66 09|                 5|\n","+-------+----------+-----------------+------------------+\n","\n","+-------+----------+-----------------+------------------+\n","|summary| Draw Date|  Winning Numbers|        Multiplier|\n","+-------+----------+-----------------+------------------+\n","|  count|      1568|             1568|              1358|\n","|   mean|      NULL|             NULL| 2.803387334315169|\n","| stddev|      NULL|             NULL|1.1846800320678832|\n","|    min|01/01/2011|01 02 03 07 39 25|                10|\n","|    max|12/31/2022|52 58 59 64 66 09|                 5|\n","+-------+----------+-----------------+------------------+\n","\n","+-------+----------+-----------------+------------------+\n","|summary| Draw Date|  Winning Numbers|        Multiplier|\n","+-------+----------+-----------------+------------------+\n","|  count|      1568|             1568|              1358|\n","|   mean|      NULL|             NULL| 2.803387334315169|\n","| stddev|      NULL|             NULL|1.1846800320678832|\n","|    min|01/01/2011|01 02 03 07 39 25|                10|\n","|    max|12/31/2022|52 58 59 64 66 09|                 5|\n","+-------+----------+-----------------+------------------+\n","\n"]},{"data":{"text/plain":["1.3075296264141798"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["timeit.timeit(lambda : df1.describe().show(),number=5)"]},{"cell_type":"code","execution_count":24,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"9b4fb79e-b5c2-45b2-8a73-73a9a06f389a","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+-----------------+----------+\n","| Draw Date|  Winning Numbers|Multiplier|\n","+----------+-----------------+----------+\n","|12/11/2019|24 29 42 44 63 10|         4|\n","|07/20/2011|01 04 38 40 42 17|         4|\n","|01/02/2016|05 06 15 29 42 10|         2|\n","|07/14/2010|20 21 23 38 42 06|         3|\n","|06/14/2014|09 33 42 45 54 30|         3|\n","|06/15/2022|19 28 41 42 51 07|         2|\n","|06/13/2022|02 27 42 44 51 25|         2|\n","|09/07/2019|11 20 41 42 56 06|         2|\n","|01/02/2019|08 12 42 46 56 12|         2|\n","|09/04/2010|11 14 22 33 42 38|         2|\n","|09/05/2012|04 19 26 42 51 29|      NULL|\n","|10/12/2022|14 30 41 42 59 06|         5|\n","|05/12/2018|22 42 45 55 56 14|         3|\n","|04/11/2012|16 23 42 44 47 02|      NULL|\n","|06/22/2016|14 40 42 43 52 17|         3|\n","|10/17/2012|01 07 10 23 42 35|      NULL|\n","|03/08/2017|23 33 42 46 59 04|         2|\n","|07/26/2017|07 19 21 42 69 12|         2|\n","|01/18/2023|06 15 22 42 47 26|         3|\n","|03/22/2017|02 09 27 29 42 09|         2|\n","+----------+-----------------+----------+\n","only showing top 20 rows\n","\n","+----------+-----------------+----------+\n","| Draw Date|  Winning Numbers|Multiplier|\n","+----------+-----------------+----------+\n","|12/11/2019|24 29 42 44 63 10|         4|\n","|07/20/2011|01 04 38 40 42 17|         4|\n","|01/02/2016|05 06 15 29 42 10|         2|\n","|07/14/2010|20 21 23 38 42 06|         3|\n","|06/14/2014|09 33 42 45 54 30|         3|\n","|06/15/2022|19 28 41 42 51 07|         2|\n","|06/13/2022|02 27 42 44 51 25|         2|\n","|09/07/2019|11 20 41 42 56 06|         2|\n","|01/02/2019|08 12 42 46 56 12|         2|\n","|09/04/2010|11 14 22 33 42 38|         2|\n","|09/05/2012|04 19 26 42 51 29|      NULL|\n","|10/12/2022|14 30 41 42 59 06|         5|\n","|05/12/2018|22 42 45 55 56 14|         3|\n","|04/11/2012|16 23 42 44 47 02|      NULL|\n","|06/22/2016|14 40 42 43 52 17|         3|\n","|10/17/2012|01 07 10 23 42 35|      NULL|\n","|03/08/2017|23 33 42 46 59 04|         2|\n","|07/26/2017|07 19 21 42 69 12|         2|\n","|01/18/2023|06 15 22 42 47 26|         3|\n","|03/22/2017|02 09 27 29 42 09|         2|\n","+----------+-----------------+----------+\n","only showing top 20 rows\n","\n","+----------+-----------------+----------+\n","| Draw Date|  Winning Numbers|Multiplier|\n","+----------+-----------------+----------+\n","|12/11/2019|24 29 42 44 63 10|         4|\n","|07/20/2011|01 04 38 40 42 17|         4|\n","|01/02/2016|05 06 15 29 42 10|         2|\n","|07/14/2010|20 21 23 38 42 06|         3|\n","|06/14/2014|09 33 42 45 54 30|         3|\n","|06/15/2022|19 28 41 42 51 07|         2|\n","|06/13/2022|02 27 42 44 51 25|         2|\n","|09/07/2019|11 20 41 42 56 06|         2|\n","|01/02/2019|08 12 42 46 56 12|         2|\n","|09/04/2010|11 14 22 33 42 38|         2|\n","|09/05/2012|04 19 26 42 51 29|      NULL|\n","|10/12/2022|14 30 41 42 59 06|         5|\n","|05/12/2018|22 42 45 55 56 14|         3|\n","|04/11/2012|16 23 42 44 47 02|      NULL|\n","|06/22/2016|14 40 42 43 52 17|         3|\n","|10/17/2012|01 07 10 23 42 35|      NULL|\n","|03/08/2017|23 33 42 46 59 04|         2|\n","|07/26/2017|07 19 21 42 69 12|         2|\n","|01/18/2023|06 15 22 42 47 26|         3|\n","|03/22/2017|02 09 27 29 42 09|         2|\n","+----------+-----------------+----------+\n","only showing top 20 rows\n","\n","+----------+-----------------+----------+\n","| Draw Date|  Winning Numbers|Multiplier|\n","+----------+-----------------+----------+\n","|12/11/2019|24 29 42 44 63 10|         4|\n","|07/20/2011|01 04 38 40 42 17|         4|\n","|01/02/2016|05 06 15 29 42 10|         2|\n","|07/14/2010|20 21 23 38 42 06|         3|\n","|06/14/2014|09 33 42 45 54 30|         3|\n","|06/15/2022|19 28 41 42 51 07|         2|\n","|06/13/2022|02 27 42 44 51 25|         2|\n","|09/07/2019|11 20 41 42 56 06|         2|\n","|01/02/2019|08 12 42 46 56 12|         2|\n","|09/04/2010|11 14 22 33 42 38|         2|\n","|09/05/2012|04 19 26 42 51 29|      NULL|\n","|10/12/2022|14 30 41 42 59 06|         5|\n","|05/12/2018|22 42 45 55 56 14|         3|\n","|04/11/2012|16 23 42 44 47 02|      NULL|\n","|06/22/2016|14 40 42 43 52 17|         3|\n","|10/17/2012|01 07 10 23 42 35|      NULL|\n","|03/08/2017|23 33 42 46 59 04|         2|\n","|07/26/2017|07 19 21 42 69 12|         2|\n","|01/18/2023|06 15 22 42 47 26|         3|\n","|03/22/2017|02 09 27 29 42 09|         2|\n","+----------+-----------------+----------+\n","only showing top 20 rows\n","\n","+----------+-----------------+----------+\n","| Draw Date|  Winning Numbers|Multiplier|\n","+----------+-----------------+----------+\n","|12/11/2019|24 29 42 44 63 10|         4|\n","|07/20/2011|01 04 38 40 42 17|         4|\n","|01/02/2016|05 06 15 29 42 10|         2|\n","|07/14/2010|20 21 23 38 42 06|         3|\n","|06/14/2014|09 33 42 45 54 30|         3|\n","|06/15/2022|19 28 41 42 51 07|         2|\n","|06/13/2022|02 27 42 44 51 25|         2|\n","|09/07/2019|11 20 41 42 56 06|         2|\n","|01/02/2019|08 12 42 46 56 12|         2|\n","|09/04/2010|11 14 22 33 42 38|         2|\n","|09/05/2012|04 19 26 42 51 29|      NULL|\n","|10/12/2022|14 30 41 42 59 06|         5|\n","|05/12/2018|22 42 45 55 56 14|         3|\n","|04/11/2012|16 23 42 44 47 02|      NULL|\n","|06/22/2016|14 40 42 43 52 17|         3|\n","|10/17/2012|01 07 10 23 42 35|      NULL|\n","|03/08/2017|23 33 42 46 59 04|         2|\n","|07/26/2017|07 19 21 42 69 12|         2|\n","|01/18/2023|06 15 22 42 47 26|         3|\n","|03/22/2017|02 09 27 29 42 09|         2|\n","+----------+-----------------+----------+\n","only showing top 20 rows\n","\n"]},{"data":{"text/plain":["0.8134955940768123"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["timeit.timeit(lambda : df2.filter(df2[\"Winning Numbers\"].contains(\"42\")).show(), number=5)"]},{"cell_type":"code","execution_count":25,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"77d4fcb7-2a43-4fe6-b736-8918ef654c17","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+-----------------+----------+\n","| Draw Date|  Winning Numbers|Multiplier|\n","+----------+-----------------+----------+\n","|10/31/2020|02 06 40 42 55 24|         3|\n","|01/27/2021|17 33 35 42 52 09|         3|\n","|03/17/2021|34 38 42 61 62 19|         2|\n","|03/20/2021|01 06 22 42 61 04|         3|\n","|08/22/2020|19 30 36 42 66 14|         3|\n","|05/20/2020|18 34 40 42 50 09|         2|\n","|05/16/2020|08 12 26 39 42 11|         2|\n","|05/09/2020|12 18 42 48 65 19|         5|\n","|04/11/2020|22 29 30 42 47 17|         3|\n","|03/25/2020|05 09 27 39 42 16|         2|\n","|12/11/2019|24 29 42 44 63 10|         4|\n","|12/07/2019|18 42 53 62 66 25|         3|\n","|11/30/2019|15 35 42 63 68 18|         4|\n","|09/07/2019|11 20 41 42 56 06|         2|\n","|06/12/2019|05 35 38 42 57 13|         2|\n","|06/08/2019|09 13 42 48 60 18|         2|\n","|05/29/2019|03 32 34 42 61 07|         2|\n","|02/27/2019|21 31 42 49 59 23|         5|\n","|01/02/2019|08 12 42 46 56 12|         2|\n","|12/29/2018|12 42 51 53 62 25|         2|\n","+----------+-----------------+----------+\n","only showing top 20 rows\n","\n","+----------+-----------------+----------+\n","| Draw Date|  Winning Numbers|Multiplier|\n","+----------+-----------------+----------+\n","|10/31/2020|02 06 40 42 55 24|         3|\n","|01/27/2021|17 33 35 42 52 09|         3|\n","|03/17/2021|34 38 42 61 62 19|         2|\n","|03/20/2021|01 06 22 42 61 04|         3|\n","|08/22/2020|19 30 36 42 66 14|         3|\n","|05/20/2020|18 34 40 42 50 09|         2|\n","|05/16/2020|08 12 26 39 42 11|         2|\n","|05/09/2020|12 18 42 48 65 19|         5|\n","|04/11/2020|22 29 30 42 47 17|         3|\n","|03/25/2020|05 09 27 39 42 16|         2|\n","|12/11/2019|24 29 42 44 63 10|         4|\n","|12/07/2019|18 42 53 62 66 25|         3|\n","|11/30/2019|15 35 42 63 68 18|         4|\n","|09/07/2019|11 20 41 42 56 06|         2|\n","|06/12/2019|05 35 38 42 57 13|         2|\n","|06/08/2019|09 13 42 48 60 18|         2|\n","|05/29/2019|03 32 34 42 61 07|         2|\n","|02/27/2019|21 31 42 49 59 23|         5|\n","|01/02/2019|08 12 42 46 56 12|         2|\n","|12/29/2018|12 42 51 53 62 25|         2|\n","+----------+-----------------+----------+\n","only showing top 20 rows\n","\n","+----------+-----------------+----------+\n","| Draw Date|  Winning Numbers|Multiplier|\n","+----------+-----------------+----------+\n","|10/31/2020|02 06 40 42 55 24|         3|\n","|01/27/2021|17 33 35 42 52 09|         3|\n","|03/17/2021|34 38 42 61 62 19|         2|\n","|03/20/2021|01 06 22 42 61 04|         3|\n","|08/22/2020|19 30 36 42 66 14|         3|\n","|05/20/2020|18 34 40 42 50 09|         2|\n","|05/16/2020|08 12 26 39 42 11|         2|\n","|05/09/2020|12 18 42 48 65 19|         5|\n","|04/11/2020|22 29 30 42 47 17|         3|\n","|03/25/2020|05 09 27 39 42 16|         2|\n","|12/11/2019|24 29 42 44 63 10|         4|\n","|12/07/2019|18 42 53 62 66 25|         3|\n","|11/30/2019|15 35 42 63 68 18|         4|\n","|09/07/2019|11 20 41 42 56 06|         2|\n","|06/12/2019|05 35 38 42 57 13|         2|\n","|06/08/2019|09 13 42 48 60 18|         2|\n","|05/29/2019|03 32 34 42 61 07|         2|\n","|02/27/2019|21 31 42 49 59 23|         5|\n","|01/02/2019|08 12 42 46 56 12|         2|\n","|12/29/2018|12 42 51 53 62 25|         2|\n","+----------+-----------------+----------+\n","only showing top 20 rows\n","\n","+----------+-----------------+----------+\n","| Draw Date|  Winning Numbers|Multiplier|\n","+----------+-----------------+----------+\n","|10/31/2020|02 06 40 42 55 24|         3|\n","|01/27/2021|17 33 35 42 52 09|         3|\n","|03/17/2021|34 38 42 61 62 19|         2|\n","|03/20/2021|01 06 22 42 61 04|         3|\n","|08/22/2020|19 30 36 42 66 14|         3|\n","|05/20/2020|18 34 40 42 50 09|         2|\n","|05/16/2020|08 12 26 39 42 11|         2|\n","|05/09/2020|12 18 42 48 65 19|         5|\n","|04/11/2020|22 29 30 42 47 17|         3|\n","|03/25/2020|05 09 27 39 42 16|         2|\n","|12/11/2019|24 29 42 44 63 10|         4|\n","|12/07/2019|18 42 53 62 66 25|         3|\n","|11/30/2019|15 35 42 63 68 18|         4|\n","|09/07/2019|11 20 41 42 56 06|         2|\n","|06/12/2019|05 35 38 42 57 13|         2|\n","|06/08/2019|09 13 42 48 60 18|         2|\n","|05/29/2019|03 32 34 42 61 07|         2|\n","|02/27/2019|21 31 42 49 59 23|         5|\n","|01/02/2019|08 12 42 46 56 12|         2|\n","|12/29/2018|12 42 51 53 62 25|         2|\n","+----------+-----------------+----------+\n","only showing top 20 rows\n","\n","+----------+-----------------+----------+\n","| Draw Date|  Winning Numbers|Multiplier|\n","+----------+-----------------+----------+\n","|10/31/2020|02 06 40 42 55 24|         3|\n","|01/27/2021|17 33 35 42 52 09|         3|\n","|03/17/2021|34 38 42 61 62 19|         2|\n","|03/20/2021|01 06 22 42 61 04|         3|\n","|08/22/2020|19 30 36 42 66 14|         3|\n","|05/20/2020|18 34 40 42 50 09|         2|\n","|05/16/2020|08 12 26 39 42 11|         2|\n","|05/09/2020|12 18 42 48 65 19|         5|\n","|04/11/2020|22 29 30 42 47 17|         3|\n","|03/25/2020|05 09 27 39 42 16|         2|\n","|12/11/2019|24 29 42 44 63 10|         4|\n","|12/07/2019|18 42 53 62 66 25|         3|\n","|11/30/2019|15 35 42 63 68 18|         4|\n","|09/07/2019|11 20 41 42 56 06|         2|\n","|06/12/2019|05 35 38 42 57 13|         2|\n","|06/08/2019|09 13 42 48 60 18|         2|\n","|05/29/2019|03 32 34 42 61 07|         2|\n","|02/27/2019|21 31 42 49 59 23|         5|\n","|01/02/2019|08 12 42 46 56 12|         2|\n","|12/29/2018|12 42 51 53 62 25|         2|\n","+----------+-----------------+----------+\n","only showing top 20 rows\n","\n"]},{"data":{"text/plain":["0.5324525805190206"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["timeit.timeit(lambda : df1.filter(df1[\"Winning Numbers\"].contains(\"42\")).show(), number=5)"]},{"cell_type":"code","execution_count":26,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f31ce932-0e3b-4a4f-bd0c-664935ad6d8b","showTitle":false,"title":""}},"outputs":[],"source":["c = df2[\"Winning Numbers\"].contains(\"42\")"]},{"cell_type":"code","execution_count":27,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"4bed2f74-5010-4e7b-a925-b27219e57060","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["pyspark.sql.column.Column"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["type(c)"]},{"cell_type":"code","execution_count":28,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"71aeb66f-80a8-4070-bd40-a545a4072fe2","showTitle":false,"title":""}},"outputs":[],"source":["df1_subset = df1.filter(df1[\"Winning Numbers\"].contains(\"42\"))"]},{"cell_type":"code","execution_count":29,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"4688cb9f-5477-4773-8bb4-ada622d45cc1","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["pyspark.sql.dataframe.DataFrame"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["type(df1_subset)"]},{"cell_type":"code","execution_count":30,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"457ba9c0-be40-4bea-be29-c0b73cca9481","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["b'(1) MapPartitionsRDD[189] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[188] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  SQLExecutionRDD[187] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[186] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[185] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  FileScanRDD[184] at javaToPython at NativeMethodAccessorImpl.java:0 []'"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["df1_subset.rdd.toDebugString()"]},{"cell_type":"code","execution_count":39,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"776ad925-24f8-4cc2-91d4-e90344700f03","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Draw Date: string (nullable = true)\n"," |-- Winning Numbers: string (nullable = true)\n"," |-- Multiplier: double (nullable = true)\n","\n"]},{"data":{"text/plain":["[2.0]"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["from pyspark.sql.types import DoubleType\n","df1.printSchema()\n","df1 = df1.withColumn(\"Multiplier\", df1[\"Multiplier\"].cast(DoubleType()))\n","df1.approxQuantile(\"Multiplier\",[0.5], 0.25)"]},{"cell_type":"code","execution_count":35,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"94004616-1468-4440-9964-d41510bd3be5","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Draw Date: string (nullable = true)\n"," |-- Winning Numbers: string (nullable = true)\n"," |-- Multiplier: string (nullable = true)\n","\n"]}],"source":["df2.printSchema()"]},{"cell_type":"code","execution_count":36,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"fca2b61a-dd1e-4a6e-80d8-a22b01662858","showTitle":false,"title":""}},"outputs":[],"source":["df1_v2 = df1.withColumn(\"Multiplier\",df1[\"Multiplier\"].cast(\"int\"))"]},{"cell_type":"code","execution_count":37,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"2336723a-4823-4b69-9c16-d87a412199ef","showTitle":false,"title":""}},"outputs":[],"source":["df2_v2 = df2.withColumn(\"Multiplier\",df2[\"Multiplier\"].cast(\"int\"))"]},{"cell_type":"code","execution_count":38,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6235942a-6b32-4e3f-a063-890402cbc485","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Draw Date: string (nullable = true)\n"," |-- Winning Numbers: string (nullable = true)\n"," |-- Multiplier: integer (nullable = true)\n","\n"]}],"source":["df1_v2.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"8a25a4ac-a5ea-4c6e-89e3-c1d927a21faf","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[33]: [2.0]</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[33]: [2.0]</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["df1_v2.approxQuantile(\"Multiplier\",[0.5], 0.25)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"aad5e60e-f81c-4fc9-a6c7-2ac93f98507b","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Out[34]: [2.0]</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Out[34]: [2.0]</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["df2_v2.approxQuantile(\"Multiplier\",[0.5], 0.25)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"81f08a02-6c30-45cd-a7d2-e6ebb3f4cf8d","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Help on method approxQuantile in module pyspark.sql.dataframe:\n","\n","approxQuantile(col, probabilities, relativeError) method of pyspark.sql.dataframe.DataFrame instance\n","    Calculates the approximate quantiles of numerical columns of a\n","    :class:`DataFrame`.\n","    \n","    The result of this algorithm has the following deterministic bound:\n","    If the :class:`DataFrame` has N elements and if we request the quantile at\n","    probability `p` up to error `err`, then the algorithm will return\n","    a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n","    close to (p * N). More precisely,\n","    \n","      floor((p - err) * N) &lt;= rank(x) &lt;= ceil((p + err) * N).\n","    \n","    This method implements a variation of the Greenwald-Khanna\n","    algorithm (with some speed optimizations). The algorithm was first\n","    present in [[https://doi.org/10.1145/375663.375670\n","    Space-efficient Online Computation of Quantile Summaries]]\n","    by Greenwald and Khanna.\n","    \n","    Note that null values will be ignored in numerical columns before calculation.\n","    For columns only containing null values, an empty list is returned.\n","    \n","    .. versionadded:: 2.0.0\n","    \n","    Parameters\n","    ----------\n","    col: str, tuple or list\n","        Can be a single column name, or a list of names for multiple columns.\n","    \n","        .. versionchanged:: 2.2\n","           Added support for multiple columns.\n","    probabilities : list or tuple\n","        a list of quantile probabilities\n","        Each number must belong to [0, 1].\n","        For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n","    relativeError : float\n","        The relative target precision to achieve\n","        (&gt;= 0). If set to zero, the exact quantiles are computed, which\n","        could be very expensive. Note that values greater than 1 are\n","        accepted but give the same result as 1.\n","    \n","    Returns\n","    -------\n","    list\n","        the approximate quantiles at the given probabilities. If\n","        the input `col` is a string, the output is a list of floats. If the\n","        input `col` is a list or tuple of strings, the output is also a\n","        list, but each element in it is a list of floats, i.e., the output\n","        is a list of list of floats.\n","\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Help on method approxQuantile in module pyspark.sql.dataframe:\n\napproxQuantile(col, probabilities, relativeError) method of pyspark.sql.dataframe.DataFrame instance\n    Calculates the approximate quantiles of numerical columns of a\n    :class:`DataFrame`.\n    \n    The result of this algorithm has the following deterministic bound:\n    If the :class:`DataFrame` has N elements and if we request the quantile at\n    probability `p` up to error `err`, then the algorithm will return\n    a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n    close to (p * N). More precisely,\n    \n      floor((p - err) * N) &lt;= rank(x) &lt;= ceil((p + err) * N).\n    \n    This method implements a variation of the Greenwald-Khanna\n    algorithm (with some speed optimizations). The algorithm was first\n    present in [[https://doi.org/10.1145/375663.375670\n    Space-efficient Online Computation of Quantile Summaries]]\n    by Greenwald and Khanna.\n    \n    Note that null values will be ignored in numerical columns before calculation.\n    For columns only containing null values, an empty list is returned.\n    \n    .. versionadded:: 2.0.0\n    \n    Parameters\n    ----------\n    col: str, tuple or list\n        Can be a single column name, or a list of names for multiple columns.\n    \n        .. versionchanged:: 2.2\n           Added support for multiple columns.\n    probabilities : list or tuple\n        a list of quantile probabilities\n        Each number must belong to [0, 1].\n        For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n    relativeError : float\n        The relative target precision to achieve\n        (&gt;= 0). If set to zero, the exact quantiles are computed, which\n        could be very expensive. Note that values greater than 1 are\n        accepted but give the same result as 1.\n    \n    Returns\n    -------\n    list\n        the approximate quantiles at the given probabilities. If\n        the input `col` is a string, the output is a list of floats. If the\n        input `col` is a list or tuple of strings, the output is also a\n        list, but each element in it is a list of floats, i.e., the output\n        is a list of list of floats.\n\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["help(df1.approxQuantile)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"903d2632-4c05-46ef-ac79-87a3dae4e83e","showTitle":false,"title":""}},"outputs":[],"source":[]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"Apache Spark Getting Started","notebookOrigID":1559579,"widgets":{}},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
